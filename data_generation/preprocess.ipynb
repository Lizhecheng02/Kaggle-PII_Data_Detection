{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\86183\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import regex as re\n",
    "import random\n",
    "import os\n",
    "from faker import Faker\n",
    "from tqdm import tqdm\n",
    "from spacy.lang.en import English\n",
    "fake = Faker(\"en_US\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13994, 1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def concat_csv(directory_path):\n",
    "    dataframes = []\n",
    "\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "            dataframes.append(df)\n",
    "\n",
    "    concatenated_df = pd.concat(dataframes, ignore_index=True)\n",
    "    concatenated_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return concatenated_df\n",
    "\n",
    "directory_path = \"./gpt4-faker/\"\n",
    "df = concat_csv(directory_path)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>full_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>trailing_whitespace</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [document, full_text, tokens, trailing_whitespace, labels]\n",
       "Index: []"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df = pd.DataFrame(\n",
    "    columns=[\"document\", \"full_text\", \"tokens\", \"trailing_whitespace\", \"labels\"]\n",
    ")\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Phones\\n\\nModern humans today are always on th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This essay will explain if drivers should or s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Driving while the use of cellular devices\\n\\nT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Phones &amp; Driving\\n\\nDrivers should not be able...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cell Phone Operation While Driving\\n\\nThe abil...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           full_text\n",
       "0  Phones\\n\\nModern humans today are always on th...\n",
       "1  This essay will explain if drivers should or s...\n",
       "2  Driving while the use of cellular devices\\n\\nT...\n",
       "3  Phones & Driving\\n\\nDrivers should not be able...\n",
       "4  Cell Phone Operation While Driving\\n\\nThe abil..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME_STUDENT, He is a great student. \n",
      "USERNAME\n",
      "EMAIL\n"
     ]
    }
   ],
   "source": [
    "def convert_chinese_to_english(text):\n",
    "    punctuations = {\n",
    "        ord('，'): ', ', ord('。'): '.', ord('！'): '!', \n",
    "        ord('？'): '?', ord('：'): ':', ord('；'): ';',\n",
    "        ord('“'): '\"', ord('”'): '\"', ord('‘'): \"'\", \n",
    "        ord('’'): \"'\", ord('（'): '(', ord('）'): ')',\n",
    "        ord('【'): '[', ord('】'): ']', ord('《'): '<', \n",
    "        ord('》'): '>'\n",
    "    }\n",
    "    text = text.translate(punctuations)\n",
    "\n",
    "    def full_to_half(match):\n",
    "        return chr(ord(match.group(0)) - 0xFEE0)\n",
    "\n",
    "    text = re.sub(r'[Ａ-Ｚａ-ｚ０-９]', full_to_half, text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def format_text(input_text):\n",
    "    pattern = r'\\[.*?([A-Z_]+).*?\\].*?\\[.*?\\1.*?\\]'\n",
    "    replaced_text = re.sub(pattern, lambda m: m.group(1), input_text)\n",
    "    replaced_text = re.sub(r'([.?!])(?![\\s])', r'\\1 ', replaced_text)\n",
    "    return replaced_text\n",
    "\n",
    "input_text1 = \"[+NAME_STUDENT]Emily Thompson[NAME_STUDENT], He is a great student.\"\n",
    "input_text2 = \"[USERNAME]TravelBuddy22[-USERNAME]\"\n",
    "input_text3 = \"[EMAIL]emilythompson@example.com[*EMAIL]\"\n",
    "\n",
    "print(format_text(input_text1))  \n",
    "print(format_text(input_text2))  \n",
    "print(format_text(input_text3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['(', '682', ')', '387', '-', '3863x23756', 'and', 'James', 'Kitty'], 'trailing_whitespace': [False, False, True, False, False, True, True, True, False]}\n"
     ]
    }
   ],
   "source": [
    "en_tokenizer = English().tokenizer\n",
    "\n",
    "def tokenize_with_spacy(text, tokenizer=en_tokenizer):\n",
    "    tokenized_text = tokenizer(text)\n",
    "    tokens = [token.text for token in tokenized_text]\n",
    "    trailing_whitespace = [bool(token.whitespace_) for token in tokenized_text]\n",
    "    return {\"tokens\": tokens, \"trailing_whitespace\": trailing_whitespace}\n",
    "\n",
    "print(tokenize_with_spacy(\"(682) 387-3863x23756 and James Kitty\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fake_social_media_urls(num_urls=1):\n",
    "    social_media_platforms = {\n",
    "        \"LinkedIn\": \"linkedin.com/in/\",\n",
    "        \"YouTube\": \"youtube.com/c/\",\n",
    "        \"Instagram\": \"instagram.com/\",\n",
    "        \"GitHub\": \"github.com/\",\n",
    "        \"Facebook\": \"facebook.com/\",\n",
    "        \"Twitter\": \"twitter.com/\",\n",
    "        \"Pinterest\": \"pinterest.com/\",\n",
    "        \"Snapchat\": \"snapchat.com/\",\n",
    "        \"TikTok\": \"tiktok.com/\",\n",
    "        \"Reddit\": \"reddit.com/\",\n",
    "        \"Tumblr\": \"tumblr.com/\",\n",
    "        \"WhatsApp\": \"whatsapp.com/\",\n",
    "        \"Discord\": \"discord.com/\",\n",
    "        \"Twitch\": \"twitch.tv/\",\n",
    "        \"Vimeo\": \"vimeo.com/\",\n",
    "        \"Flickr\": \"flickr.com/\",\n",
    "        \"Telegram\": \"telegram.org/\",\n",
    "        \"Line\": \"line.me/\",\n",
    "        \"Viber\": \"viber.com/\"\n",
    "    }\n",
    "\n",
    "    fake_social_media_urls = []\n",
    "\n",
    "    for _ in range(num_urls):\n",
    "        fake_user_name = fake.user_name()\n",
    "        platform, domain = random.choice(list(social_media_platforms.items()))\n",
    "        fake_url = f\"https://{domain}{fake_user_name}\"\n",
    "        fake_social_media_urls.append(fake_url)\n",
    "\n",
    "    return fake_social_media_urls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 11229/13994 [1:49:03<26:51,  1.72it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 202\u001b[0m\n\u001b[0;32m    193\u001b[0m new_row \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;28mstr\u001b[39m(idx)],\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull_text\u001b[39m\u001b[38;5;124m\"\u001b[39m: [new_full_text],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m: [new_labels]\n\u001b[0;32m    199\u001b[0m })\n\u001b[0;32m    201\u001b[0m final_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([final_df, new_row], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 202\u001b[0m \u001b[43mfinal_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_json\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlzc_persuade_2.0_based_0218.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrecords\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_ascii\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\86183\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:2629\u001b[0m, in \u001b[0;36mNDFrame.to_json\u001b[1;34m(self, path_or_buf, orient, date_format, double_precision, force_ascii, date_unit, default_handler, lines, compression, index, indent, storage_options, mode)\u001b[0m\n\u001b[0;32m   2626\u001b[0m config\u001b[38;5;241m.\u001b[39mis_nonnegative_int(indent)\n\u001b[0;32m   2627\u001b[0m indent \u001b[38;5;241m=\u001b[39m indent \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 2629\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_json\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2630\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2631\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2632\u001b[0m \u001b[43m    \u001b[49m\u001b[43morient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2633\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2634\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdouble_precision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdouble_precision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2635\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_ascii\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_ascii\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2636\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_unit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2637\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdefault_handler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_handler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2638\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2639\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2640\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2641\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2642\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2643\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2644\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\86183\\anaconda3\\lib\\site-packages\\pandas\\io\\json\\_json.py:215\u001b[0m, in \u001b[0;36mto_json\u001b[1;34m(path_or_buf, obj, orient, date_format, double_precision, force_ascii, date_unit, default_handler, lines, compression, index, indent, storage_options, mode)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path_or_buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[0;32m    213\u001b[0m         path_or_buf, mode, compression\u001b[38;5;241m=\u001b[39mcompression, storage_options\u001b[38;5;241m=\u001b[39mstorage_options\n\u001b[0;32m    214\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m--> 215\u001b[0m         \u001b[43mhandles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m s\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# df = df[:10]\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    full_text = row[\"full_text\"]\n",
    "    # full_text = convert_text(full_text)\n",
    "    try:\n",
    "        full_text = convert_chinese_to_english(full_text)\n",
    "        full_text = format_text(full_text)\n",
    "    except:\n",
    "        # print(full_text)\n",
    "        continue\n",
    "\n",
    "    # print(full_text)\n",
    "\n",
    "    tokens = [token.text for token in en_tokenizer(full_text)]\n",
    "    trailing_whitespace = [bool(token.whitespace_) for token in en_tokenizer(full_text)]\n",
    "\n",
    "    label_names = [\"NAME_STUDENT\", \"EMAIL\", \"USERNAME\", \"ID_NUM\", \"PHONE_NUM\", \"URL_PERSSONAL\", \"STREET_ADDRESS\"]\n",
    "\n",
    "    new_tokens = []\n",
    "    new_labels = []\n",
    "    new_trailing_whitespace = []\n",
    "\n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i] not in label_names:\n",
    "            new_tokens.append(tokens[i])\n",
    "            new_labels.append(\"O\")\n",
    "            new_trailing_whitespace.append(trailing_whitespace[i])\n",
    "        else:\n",
    "            # print(1)\n",
    "            if tokens[i] == \"NAME_STUDENT\":\n",
    "                name = random.choices(\n",
    "                    [fake.name, fake.first_name, fake.last_name],\n",
    "                    weights=[0.80, 0.10, 0.10],\n",
    "                    k=1\n",
    "                )[0]()\n",
    "                # print(name)\n",
    "                name_tokens = [name_token.text for name_token in en_tokenizer(name)]\n",
    "                tmp_trailing_whitespace = [bool(name_token.whitespace_) for name_token in en_tokenizer(name)]\n",
    "                if len(name_tokens) > 1:\n",
    "                    new_tokens.extend(name_tokens)\n",
    "                    new_labels.extend([\"B-NAME_STUDENT\"] + [\"I-NAME_STUDENT\"] * (len(name_tokens) - 1))\n",
    "                    new_trailing_whitespace.extend(tmp_trailing_whitespace[:len(name_tokens) - 1])\n",
    "                    new_trailing_whitespace.append(trailing_whitespace[i])\n",
    "                else:\n",
    "                    new_tokens.extend(name_tokens)\n",
    "                    new_labels.extend([\"B-NAME_STUDENT\"])\n",
    "                    new_trailing_whitespace.append(trailing_whitespace[i])\n",
    "\n",
    "            elif tokens[i] == \"EMAIL\":\n",
    "                email = fake.ascii_free_email()\n",
    "                # print(email)\n",
    "                email_tokens = [email_token.text for email_token in en_tokenizer(email)]\n",
    "                tmp_trailing_whitespace = [bool(email_token.whitespace_) for email_token in en_tokenizer(email)]\n",
    "                if len(email_tokens) > 1:\n",
    "                    new_tokens.extend(email_tokens)\n",
    "                    new_labels.extend([\"B-EMAIL\"] + [\"I-EMAIL\"] * (len(email_tokens) - 1))\n",
    "                    new_trailing_whitespace.extend(tmp_trailing_whitespace[:len(email_tokens) - 1])\n",
    "                    new_trailing_whitespace.append(trailing_whitespace[i])\n",
    "                else:\n",
    "                    new_tokens.extend(email_tokens)\n",
    "                    new_labels.extend([\"B-EMAIL\"])\n",
    "                    new_trailing_whitespace.append(trailing_whitespace[i])\n",
    "\n",
    "            elif tokens[i] == \"USERNAME\":\n",
    "                username = fake.user_name()\n",
    "                # print(username)\n",
    "                username_tokens = [username_token.text for username_token in en_tokenizer(username)]\n",
    "                tmp_trailing_whitespace = [bool(username_token.whitespace_) for username_token in en_tokenizer(username)]\n",
    "                if len(username_tokens) > 1:\n",
    "                    new_tokens.extend(username_tokens)\n",
    "                    new_labels.extend([\"B-USERNAME\"] + [\"I-USERNAME\"] * (len(username_tokens) - 1))\n",
    "                    new_trailing_whitespace.extend(tmp_trailing_whitespace[:len(username_tokens) - 1])\n",
    "                    new_trailing_whitespace.append(trailing_whitespace[i])\n",
    "                else:\n",
    "                    new_tokens.extend(username_tokens)\n",
    "                    new_labels.extend([\"B-USERNAME\"])\n",
    "                    new_trailing_whitespace.append(trailing_whitespace[i])\n",
    "\n",
    "            elif tokens[i] == \"ID_NUM\":\n",
    "                id_num = random.choices(\n",
    "                    [fake.ssn, fake.passport_number, fake.bban, fake.iban, fake.license_plate],\n",
    "                    weights=[0.20, 0.20, 0.20, 0.20, 0.20],\n",
    "                    k=1\n",
    "                )[0]()\n",
    "                # print(id_num)\n",
    "                id_num_tokens = [id_num_token.text for id_num_token in en_tokenizer(id_num)]\n",
    "                tmp_trailing_whitespace = [bool(id_num_token.whitespace_) for id_num_token in en_tokenizer(id_num)]\n",
    "                if len(id_num_tokens) > 1:\n",
    "                    new_tokens.extend(id_num_tokens)\n",
    "                    new_labels.extend([\"B-ID_NUM\"] + [\"I-ID_NUM\"] * (len(id_num_tokens) - 1))\n",
    "                    new_trailing_whitespace.extend(tmp_trailing_whitespace[:len(id_num_tokens) - 1])\n",
    "                    new_trailing_whitespace.append(trailing_whitespace[i])\n",
    "                else:\n",
    "                    new_tokens.extend(id_num_tokens)\n",
    "                    new_labels.extend([\"B-ID_NUM\"])\n",
    "                    new_trailing_whitespace.append(trailing_whitespace[i])\n",
    "\n",
    "            elif tokens[i] == \"PHONE_NUM\":\n",
    "                phone_number = fake.phone_number()\n",
    "                # print(phone_number)\n",
    "                phone_number_tokens = [phone_number_token.text for phone_number_token in en_tokenizer(phone_number)]\n",
    "                tmp_trailing_whitespace = [bool(phone_number_token.whitespace_) for phone_number_token in en_tokenizer(phone_number)]\n",
    "                if len(phone_number_tokens) > 1:\n",
    "                    new_tokens.extend(phone_number_tokens)\n",
    "\n",
    "                    # tmp_new_labels = []\n",
    "                    # flag = False\n",
    "                    # for j in range(len(phone_number_tokens)):\n",
    "                    #     if phone_number_tokens[j] in [\"(\", \")\", \"-\", \",\", \"+\", \".\"]:\n",
    "                    #         tmp_new_labels.append(\"O\")\n",
    "                    #     else:\n",
    "                    #         if not flag:\n",
    "                    #             tmp_new_labels.append(\"B-PHONE_NUM\")\n",
    "                    #             flag = True\n",
    "                    #         else:\n",
    "                    #             tmp_new_labels.append(\"I-PHONE_NUM\")\n",
    "\n",
    "                    # new_labels.extend(tmp_new_labels)\n",
    "\n",
    "                    new_labels.extend([\"B-PHONE_NUM\"] + [\"I-PHONE_NUM\"] * (len(phone_number_tokens) - 1))\n",
    "                    new_trailing_whitespace.extend(tmp_trailing_whitespace[:len(phone_number_tokens) - 1])\n",
    "                    new_trailing_whitespace.append(trailing_whitespace[i])\n",
    "                else:\n",
    "                    new_tokens.extend(phone_number_tokens)\n",
    "                    new_labels.extend([\"B-PHONE_NUM\"])\n",
    "                    new_trailing_whitespace.append(trailing_whitespace[i])\n",
    "\n",
    "            elif tokens[i] == \"URL_PERSONAL\":\n",
    "                url_personal = generate_fake_social_media_urls(1)\n",
    "                # print(url_personal)\n",
    "                url_personal_tokens = [url_personal_token.text for url_personal_token in en_tokenizer(url_personal)]\n",
    "                tmp_trailing_whitespace = [bool(url_personal_token.whitespace_) for url_personal_token in en_tokenizer(url_personal)]\n",
    "                if len(url_personal_tokens) > 1:\n",
    "                    new_tokens.extend(url_personal_tokens)\n",
    "\n",
    "                    # tmp_new_labels = []\n",
    "                    # flag = False\n",
    "                    # for j in range(len(url_personal_tokens)):\n",
    "                    #     if url_personal_tokens[j] in [\"(\", \")\", \"-\", \",\"]:\n",
    "                    #         tmp_new_labels.append(\"O\")\n",
    "                    #     else:\n",
    "                    #         if not flag:\n",
    "                    #             tmp_new_labels.append(\"B-URL_PERSONAL\")\n",
    "                    #             flag = True\n",
    "                    #         else:\n",
    "                    #             tmp_new_labels.append(\"I-URL_PERSONAL\")\n",
    "\n",
    "                    new_labels.extend([\"B-URL_PERSONAL\"] + [\"I-URL_PERSONAL\"] * (len(url_personal_tokens) - 1))\n",
    "                    new_trailing_whitespace.extend(tmp_trailing_whitespace[:len(url_personal_tokens) - 1])\n",
    "                    new_trailing_whitespace.append(trailing_whitespace[i])\n",
    "                else:\n",
    "                    new_tokens.extend(url_personal_tokens)\n",
    "                    new_labels.extend([\"B-URL_PERSONAL\"])\n",
    "                    new_trailing_whitespace.extend(trailing_whitespace[i])\n",
    "\n",
    "            elif tokens[i] == \"STREET_ADDRESS\":\n",
    "                street_address = fake.address()\n",
    "                # print(street_address)\n",
    "                street_address_tokens = [street_address_token.text for street_address_token in en_tokenizer(street_address)]\n",
    "                tmp_trailing_whitespace = [bool(street_address_token.whitespace_) for street_address_token in en_tokenizer(street_address)]\n",
    "\n",
    "                if len(street_address_tokens) > 1:\n",
    "                    new_tokens.extend(street_address_tokens)\n",
    "\n",
    "                    # tmp_new_labels = []\n",
    "                    # flag = False\n",
    "                    # for j in range(len(street_address_tokens)):\n",
    "                    #     if street_address_tokens[j] in [\"(\", \")\", \"-\", \",\"]:\n",
    "                    #         tmp_new_labels.append(\"O\")\n",
    "                    #     else:\n",
    "                    #         if not flag:\n",
    "                    #             tmp_new_labels.append(\"B-STREET_ADDRESS\")\n",
    "                    #             flag = True\n",
    "                    #         else:\n",
    "                    #             tmp_new_labels.append(\"I-STREET_ADDRESS\")\n",
    "\n",
    "                    new_labels.extend([\"B-STREET_ADDRESS\"] + [\"I-STREET_ADDRESS\"] * (len(street_address_tokens) - 1))\n",
    "                    new_trailing_whitespace.extend(tmp_trailing_whitespace[:len(street_address_tokens) - 1])\n",
    "                    new_trailing_whitespace.append(trailing_whitespace[i])\n",
    "                else:\n",
    "                    new_tokens.extend(street_address_tokens)\n",
    "                    new_labels.extend([\"B-STREET_ADDRESS\"])\n",
    "                    new_trailing_whitespace.extend(trailing_whitespace[i])\n",
    "\n",
    "    if len(new_labels) != len(new_trailing_whitespace) or len(new_labels) != len(new_tokens):\n",
    "        print(\"The lengths of different columns are not equal!!!\")\n",
    "    # else:\n",
    "    #     print(len(new_labels))\n",
    "\n",
    "    new_full_text = \"\".join([token + \" \" * space for token, space in zip(new_tokens, new_trailing_whitespace)])\n",
    "\n",
    "    new_row = pd.DataFrame({\n",
    "        \"document\": [str(idx)],\n",
    "        \"full_text\": [new_full_text],\n",
    "        \"tokens\": [new_tokens],\n",
    "        \"trailing_whitespace\": [new_trailing_whitespace],\n",
    "        \"labels\": [new_labels]\n",
    "    })\n",
    "\n",
    "    final_df = pd.concat([final_df, new_row], ignore_index=True)\n",
    "    final_df.to_json(\"lzc_persuade_2.0_based_0218.json\", orient=\"records\", force_ascii=False)\n",
    "    \n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
