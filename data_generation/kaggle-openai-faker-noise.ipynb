{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:From c:\\Users\\86183\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n","\n"]}],"source":["import os\n","import pandas as pd\n","import string\n","import random\n","import json\n","import string\n","from faker import Faker\n","from spacy.lang.en import English\n","from tqdm.auto import tqdm\n","from openai import OpenAI\n","\n","os.environ[\"OPENAI_API_KEY\"] = \"\""]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["fake = Faker(\"en_US\")\n","en_tokenizer = English().tokenizer\n","\n","\n","def tokenize_with_spacy(text, tokenizer=en_tokenizer):\n","    tokenized_text = tokenizer(text)\n","    tokens = [token.text for token in tokenized_text]\n","    trailing_whitespace = [bool(token.whitespace_) for token in tokenized_text]\n","    return {\"tokens\": tokens, \"trailing_whitespace\": trailing_whitespace}\n","\n","\n","def pj_to_pj(row):\n","    labeldict = {}\n","    labels_ = row[\"labeldict\"]\n","\n","    for k, v in labels_.items():\n","        v_processed = []\n","\n","        for x in v:\n","            # This splits entities such as \"Aaron Smith\" to \"Aaron\", \"Smith\"\n","            x = x.split(\" \")\n","            for n in x:\n","                while n[-1] in string.punctuation:\n","                    n = n[:-1]\n","                v_processed += [n]\n","\n","            # Sadly, we need a special rule for some phone numbers containing \"-\"\n","            # Some numbers are such as +1-725-834-5654x4787 are split on \"-\"\n","            if k == \"PHONE_NUM\":\n","                for x in v:\n","                    x = x.split(\"-\")\n","                    for n in x:\n","                        while n[-1] in string.punctuation:\n","                            n = n[:-1]\n","                        v_processed += [n]\n","        labeldict[k] = v_processed\n","    return labeldict\n","\n","\n","def generate_fake_social_media_urls(num_urls=1):\n","    social_media_platforms = {\n","        \"LinkedIn\": \"linkedin.com/in/\",\n","        \"YouTube\": \"youtube.com/c/\",\n","        \"Instagram\": \"instagram.com/\",\n","        \"GitHub\": \"github.com/\",\n","        \"Facebook\": \"facebook.com/\",\n","        \"Twitter\": \"twitter.com/\",\n","        \"Pinterest\": \"pinterest.com/\",\n","        \"Snapchat\": \"snapchat.com/\",\n","        \"TikTok\": \"tiktok.com/\",\n","        \"Reddit\": \"reddit.com/\",\n","        \"Tumblr\": \"tumblr.com/\",\n","        \"WhatsApp\": \"whatsapp.com/\",\n","        \"Discord\": \"discord.com/\",\n","        \"Twitch\": \"twitch.tv/\",\n","        \"Vimeo\": \"vimeo.com/\",\n","        \"Flickr\": \"flickr.com/\",\n","        \"Telegram\": \"telegram.org/\",\n","        \"Line\": \"line.me/\",\n","        \"Viber\": \"viber.com/\"\n","    }\n","\n","    fake_social_media_urls = []\n","\n","    for _ in range(num_urls):\n","        fake_user_name = fake.user_name()\n","        platform, domain = random.choice(list(social_media_platforms.items()))\n","        fake_url = f\"https://{domain}{fake_user_name}\"\n","        fake_social_media_urls.append(fake_url)\n","\n","    return fake_social_media_urls[0]\n","\n","\n","def write_essay(data, client):\n","\n","    prompt = \"\"\"\n","Given some personal information about the student, like name, emai, etc as a dictionary. Write an essay in first person or third person that includes all the given information somewhere in the essay.\n","\n","The essay should detail experience of applying a specific tool or approach to address a complex challenge, and should not only narrate the process but also critically analyze the effectiveness of the chosen tool or approach, reflecting on its strengths and potential limitations.\n","\n","Every time you write this essay, you need to think of a new experience, use your imagination, but at the same time, it also needs to be consistent with reality.\n","\n","Given information has the following keys:\n","NAME_STUDENT - The full or partial name of a student that is not necessarily the author of the essay. This excludes instructors, authors, and other person names.\n","EMAIL - A student's email address.\n","USERNAME - A student\"s username on any platform.\n","ID_NUM - A number or sequence of characters that could be used to identify a student, such as a student ID or a social security number.\n","PHONE_NUM - A phone number associated with a student.\n","URL_PERSONAL - A URL that might be used to identify a student.\n","STREET_ADDRESS - A full or partial street address that is associated with the student, such as their home address.\n","\n","NAME_OTHERS - The full or partial name of instructors, authors, or other person names, and is definitely not a student name.\n","EMAIL_OTHERS - The email address of instructors, authors, or other person, and is definitely not a student email.\n","URL_OTHERS - The URL used to identify instructors, authors, or other person, and is definitely not a student URL.\n","STREET_ADDRESS_OTHERS - The street address that is associated with instructors, authors, or other person, and is definitely not a student address.\n","\n","Each of these will have a list of values associated with them.\n","\n","Below are the requirements for the essay:\n","(1) Ensure that you include each and every one of given student's personal information in the essay, do not miss out any. \n","(2) The entire essay needs to ensure coherence and logic. \n","(3) The distribution of given information within the essay should ideally be even and personal information must not appear adjacent in the essay.\n","(4) The essay must create at least one name other than students, and must cite at least one article, stating its content and author. Try to use the author's full name or the standard APA format for citation.\n","(5) The essay must create other email, URL, or street address that are not related with student.\n","(6) The essay should contain at least 900 words.\n","    \"\"\"\n","\n","    messages = [\n","        {\n","            \"role\": \"system\",\n","            \"content\": prompt\n","        },\n","        {\n","            \"role\": \"user\",\n","            \"content\": str(data)\n","        }\n","    ]\n","\n","    response = client.chat.completions.create(\n","        model=\"gpt-3.5-turbo-0125\",\n","        messages=messages,\n","        temperature=0.99\n","    )\n","\n","    return response.choices[0].message.content"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"49d35306ab914050a38c41cb73540b4c","version_major":2,"version_minor":0},"text/plain":["Generating Data:   0%|          | 0/500 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["  document                                               text      tokens  \\\n","0     pj_0  In a world constantly shaped by technological ...          In   \n","1     pj_0  In a world constantly shaped by technological ...           a   \n","2     pj_0  In a world constantly shaped by technological ...       world   \n","3     pj_0  In a world constantly shaped by technological ...  constantly   \n","4     pj_0  In a world constantly shaped by technological ...      shaped   \n","\n","  trailing_whitespace labels  \n","0                True      O  \n","1                True      O  \n","2                True      O  \n","3                True      O  \n","4                True      O  \n","O\n","I-URL_PERSONAL\n"]}],"source":["if __name__ == \"__main__\":\n","    NUM = 500\n","    fake_identities = []\n","    texts_ = []\n","    more_data = []\n","    client = OpenAI()\n","\n","    for i in tqdm(range(NUM), desc=\"Generating Data: \"):\n","        temp_d = {\n","            \"NAME_STUDENT\": [],\n","            \"EMAIL\": [],\n","            \"USERNAME\": [],\n","            \"ID_NUM\": [],\n","            \"PHONE_NUM\": [],\n","            \"URL_PERSONAL\": [],\n","            \"STREET_ADDRESS\": []\n","        }\n","        for j in range(random.choices([0, 1, 2, 3], weights=[0.005, 0.9, 0.07, 0.025], k=1)[0]):\n","            name = random.choices(\n","                [fake.name, fake.first_name, fake.last_name],\n","                weights=[0.70, 0.15, 0.15],\n","                k=1\n","            )[0]()\n","            temp_d[\"NAME_STUDENT\"].append(name)\n","\n","        for j in range(random.choices([0, 1, 2], weights=[0.05, 0.9, 0.05], k=1)[0]):\n","            temp_d[\"EMAIL\"].append(fake.ascii_free_email())\n","\n","        for j in range(random.choices([0, 1, 2, 3], weights=[0.05, 0.8, 0.1, 0.05], k=1)[0]):\n","            temp_d[\"USERNAME\"].append(fake.user_name())\n","\n","        for j in range(random.choices([0, 1, 2], weights=[0.1, 0.8, 0.1], k=1)[0]):\n","            name = random.choices(\n","                [fake.ssn, fake.passport_number, fake.bban, fake.iban, fake.license_plate],\n","                weights=[0.30, 0.30, 0.10, 0.10, 0.20],\n","                k=1\n","            )[0]()\n","            temp_d[\"ID_NUM\"].append(name)\n","\n","        for j in range(random.choices([0, 1, 2], weights=[0.05, 0.90, 0.05], k=1)[0]):\n","            temp_d[\"PHONE_NUM\"].append(fake.phone_number())\n","\n","        for j in range(random.choices([0, 1, 2], weights=[0.1, 0.8, 0.1], k=1)[0]):\n","            temp_d[\"URL_PERSONAL\"].append(generate_fake_social_media_urls(1))\n","\n","        for j in range(random.choices([0, 1, 2], weights=[0.05, 0.9, 0.05], k=1)[0]):\n","            temp_d[\"STREET_ADDRESS\"].append(fake.address())\n","\n","        fake_identities.append(temp_d)\n","        generated_text = write_essay(temp_d, client)\n","        texts_.append(generated_text)\n","\n","    df = pd.DataFrame({\n","        \"text\": texts_,\n","        \"labeldict\": fake_identities\n","    })\n","\n","    labels_ = []\n","    for i, row in df.iterrows():\n","        labeldict = pj_to_pj(row)\n","        labels_.append(labeldict)\n","\n","    tokens = [tokenize_with_spacy(r[\"text\"]) for idx, r in df.iterrows()]\n","\n","    # This loop is very inefficient, but it takes 0.3 seconds - so who cares\n","    new_ = []\n","    for tok, l in zip(tokens, labels_):\n","\n","        # these will just be forwarded to the final result, as we do not change these\n","        t = tok[\"tokens\"]\n","        ws = tok[\"trailing_whitespace\"]\n","\n","        # Create \"O\" label as standard value to overwrite on specific indices\n","        new_labels = [\"O\"] * len(t)\n","\n","        # Find entities from labels_ in the text\n","        for ent_type, ent_list in l.items():\n","            for ent_ in ent_list:\n","                # find occurence of tagged entities in the list\n","                # - this assumes that entities are not containing commong words such as \"the\"\n","                indices = [i for i, x in enumerate(t) if x == ent_]\n","                for i in indices:\n","                    # overwrite \"O\" label with correct label\n","                    new_labels[i] = ent_type\n","\n","        new_.append({\n","            \"tokens\": t,\n","            \"trailing_whitespace\": ws,\n","            \"labels\": new_labels\n","        })\n","\n","    # As we only labelled words, but not punctuation inbetween these words, we need to fill the gaps\n","    new_2 = []\n","    punctuation = [p for p in string.punctuation]\n","    for r, labeldict in zip(new_, labels_):\n","\n","        sandwich_on_comma = [\"STREET_ADDRESS\"]\n","        # again these are just forwarded\n","        t = r[\"tokens\"]\n","        ws = r[\"trailing_whitespace\"]\n","        # again, these may get overwritten\n","        label = r[\"labels\"]\n","        new_labels = [\"O\"] * len(label)\n","        for i, l in enumerate(label):\n","            # get prior label if possible\n","            if i != 0:\n","                prior_label = label[i - 1]\n","            else:\n","                prior_label = \"O\"\n","\n","            # get next label\n","            if i + 1 < len(label):\n","                next_label = label[i + 1]\n","            elif i + 1 == len(label):\n","                next_label = \"O\"\n","\n","            # skip filler / list words that split multiple entities\n","            if (t[i] == \"and\" and l == \"O\") or (t[i] == \"or\" and l == \"O\"):\n","                new_labels[i] = \"O\"\n","\n","            elif prior_label == \"EMAIL\" and t[i] == \"to\":\n","                new_labels[i] = \"O\"\n","\n","            # only street addresses should contain commas - this avoids labelling sandwiches\n","            # which chain multiple entities, such as \"Valentin Werner, Thomas Müller, and Manuel Neuer\"\n","            # As these should be three separate entities\n","            elif t[i] == \",\" and prior_label not in sandwich_on_comma:\n","                new_labels[i] = \"O\"\n","\n","            # replace if we got a sandwich (\"LABEL\"-\"O\"-\"LABEL\", such as \"Berlin\" - \",\" - \"Germany\")\n","            elif prior_label == next_label and prior_label != \"O\":\n","                new_labels[i] = prior_label\n","            elif l != \"O\":\n","                new_labels[i] = l\n","            else:\n","                new_labels[i] = \"O\"\n","\n","        new_2.append({\n","            \"tokens\": t,\n","            \"trailing_whitespace\": ws,\n","            \"labels\": new_labels\n","        })\n","\n","    # Turn labels into BIO Labels\n","    new_bio = []\n","    for i, r in enumerate(new_2):\n","\n","        # again, these are just forwarded\n","        t = r[\"tokens\"]\n","        ws = r[\"trailing_whitespace\"]\n","        # again, these might get overwritten\n","        label = r[\"labels\"]\n","\n","        # keep track of last label to identify when to use B or I\n","        last_label = \"O\"\n","        for i, l in enumerate(label):\n","            if l != last_label and l != \"O\":\n","                label[i] = \"B-\" + l\n","            elif l == last_label and last_label != \"O\":\n","                label[i] = \"I-\" + l\n","            last_label = l\n","        new_bio.append({\n","            \"doc_prelim\": i,\n","            \"tokens\": t,\n","            \"trailing_whitespace\": ws,\n","            \"labels\": label\n","        })\n","\n","    new_ = pd.DataFrame(new_bio)\n","    new_ = new_.explode([\"tokens\", \"trailing_whitespace\", \"labels\"])\n","    new_.shape  # note that this produces even more tokens than my prior approach\n","\n","    # new_ = new_.reset_index(names=\"doc_\")\n","    new_ = new_.reset_index().rename(columns={\"index\": \"doc_\"})\n","\n","    new_[\"document\"] = new_.doc_.apply(lambda x: f\"pj_{x}\")\n","\n","    # get text from original\n","    new_ = pd.merge(new_, df.reset_index().rename(columns={\"index\": \"doc_\"})[[\"doc_\", \"text\"]], on=\"doc_\", how=\"left\")\n","    new_ = new_.drop(columns=[\"doc_prelim\", \"doc_\"])\n","    new_ = new_.rename(columns={\"text_x\": \"text\"})\n","    new_ = new_[[\"document\", \"text\", \"tokens\", \"trailing_whitespace\", \"labels\"]]\n","    print(new_.head(5))\n","\n","    # Sanity checks¶\n","    target = [\n","        \"B-EMAIL\", \"B-ID_NUM\", \"B-NAME_STUDENT\", \"B-PHONE_NUM\",\n","        \"B-STREET_ADDRESS\", \"B-URL_PERSONAL\", \"B-USERNAME\", \"I-ID_NUM\",\n","        \"I-NAME_STUDENT\", \"I-PHONE_NUM\", \"I-STREET_ADDRESS\", \"I-URL_PERSONAL\"\n","    ]\n","\n","    for l in new_.labels.unique():\n","        if l not in target:\n","            print(l)\n","\n","    for l in target:\n","        if l not in new_.labels.unique():\n","            print(l)\n","\n","    # unexplode columns to lists (technically you can skip this step and safe it exploded instead too)\n","    fixed = (\n","        new_.groupby(\"document\")\n","        .agg(\n","            {\n","                \"text\": lambda x: x,\n","                \"tokens\": lambda x: x.tolist(),\n","                \"trailing_whitespace\": lambda x: x.tolist(),\n","                \"labels\": lambda x: x.tolist(),\n","            }\n","        )\n","        .reset_index()\n","    )\n","    # fix text that was turned into list\n","    fixed[\"text\"] = fixed.text.apply(lambda x: x[0])\n","    json_format = []\n","    for idx, row in fixed.iterrows():\n","        doc = row[\"document\"]\n","        text = row[\"text\"]\n","        tokens = row[\"tokens\"]\n","        ws = row[\"trailing_whitespace\"]\n","        labels = row[\"labels\"]\n","\n","        json_format.append({\n","            \"document\": doc,\n","            \"full_text\": text,\n","            \"tokens\": tokens,\n","            \"trailing_whitespace\": ws,\n","            \"labels\": labels\n","        })\n","\n","    out_file = open(\"./noise-openai-faker/noise_data_test3.json\", \"w\")\n","    json.dump(json_format, out_file)\n","    out_file.close()"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":7500999,"sourceId":66653,"sourceType":"competition"}],"dockerImageVersionId":30646,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":4}
