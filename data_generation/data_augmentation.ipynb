{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\86183\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['http://www.youtube.com/lizhecheng/hdkahdkahdikagk4784279hdkadhdsadadadaXilfshfl'], 'trailing_whitespace': [False]}\n"
     ]
    }
   ],
   "source": [
    "en_tokenizer = English().tokenizer\n",
    "\n",
    "def tokenize_with_spacy(text, tokenizer=en_tokenizer):\n",
    "    tokenized_text = tokenizer(text)\n",
    "    tokens = [token.text for token in tokenized_text]\n",
    "    trailing_whitespace = [bool(token.whitespace_) for token in tokenized_text]\n",
    "    return {\"tokens\": tokens, \"trailing_whitespace\": trailing_whitespace}\n",
    "\n",
    "print(tokenize_with_spacy(\"http://www.youtube.com/lizhecheng/hdkahdkahdikagk4784279hdkadhdsadadadaXilfshfl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  document                                          full_text  \\\n",
      "0     pj_0  In tackling the intricate challenges that ofte...   \n",
      "1     pj_1  In the realm of problem-solving, the power of ...   \n",
      "2    pj_10  In the realm of problem-solving, the utilizati...   \n",
      "3   pj_100  In the realm of problem-solving, the applicati...   \n",
      "4   pj_101  In a bustling city where innovation thrives, C...   \n",
      "\n",
      "                                              tokens  \\\n",
      "0  [In, tackling, the, intricate, challenges, tha...   \n",
      "1  [In, the, realm, of, problem, -, solving, ,, t...   \n",
      "2  [In, the, realm, of, problem, -, solving, ,, t...   \n",
      "3  [In, the, realm, of, problem, -, solving, ,, t...   \n",
      "4  [In, a, bustling, city, where, innovation, thr...   \n",
      "\n",
      "                                 trailing_whitespace  \\\n",
      "0  [True, True, True, True, True, True, True, Tru...   \n",
      "1  [True, True, True, True, False, False, False, ...   \n",
      "2  [True, True, True, True, False, False, False, ...   \n",
      "3  [True, True, True, True, False, False, False, ...   \n",
      "4  [True, True, True, True, True, True, False, Tr...   \n",
      "\n",
      "                                              labels  \n",
      "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
      "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
      "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
      "3  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
      "4  [O, O, O, O, O, O, O, O, B-NAME_STUDENT, O, O,...  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1499, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(\"./noise-openai-faker/lzc_noise_data_1500_0206.json\")\n",
    "print(df.head())\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_df = pd.DataFrame(columns=[\"document\", \"full_text\", \"tokens\", \"trailing_whitespace\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1499/1499 [00:17<00:00, 86.29it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1499, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    if len(row[\"tokens\"]) != len(row[\"trailing_whitespace\"]) or len(row[\"trailing_whitespace\"]) != len(row[\"labels\"]):\n",
    "        print(\"The lengths of different columns are not equal!\")\n",
    "    else:\n",
    "        new_tokens = []\n",
    "        new_trailing_whitespace = []\n",
    "        new_labels = []\n",
    "        new_full_text = \"\"\n",
    "\n",
    "        label_lists = [\"B-USERNAME\", \"B-ID_NUM\", \"B-EMAIL\", \"B-URL_PERSONAL\"]\n",
    "\n",
    "        total_len = len(row[\"tokens\"])\n",
    "        for i in range(total_len):\n",
    "            if row[\"labels\"][i] not in label_lists:\n",
    "                new_tokens.append(row[\"tokens\"][i])\n",
    "                new_trailing_whitespace.append(row[\"trailing_whitespace\"][i])\n",
    "                new_labels.append(row[\"labels\"][i])\n",
    "            else:\n",
    "                if i + 1 < total_len and row[\"labels\"][i + 1] != \"O\":\n",
    "                    new_tokens.append(row[\"tokens\"][i])\n",
    "                    new_trailing_whitespace.append(row[\"trailing_whitespace\"][i])\n",
    "                    new_labels.append(row[\"labels\"][i])\n",
    "                elif i + 1 < total_len and row[\"labels\"][i + 1] == \"O\":\n",
    "                    random_float = random.uniform(0, 1)\n",
    "                    if random_float <= 0.01:  # 这里我们左右加括号并且留一个空格\n",
    "                        new_tokens.extend([\"(\", row[\"tokens\"][i], \")\"])\n",
    "                        new_trailing_whitespace.extend([True, True, row[\"trailing_whitespace\"][i]])\n",
    "                        new_labels.extend([\"O\", row[\"labels\"][i], \"O\"])\n",
    "                    elif random_float <= 0.03:  # 这里我们左右加括号但是不留空格\n",
    "                        new_tokens.extend([\"(\", row[\"tokens\"][i], \")\"])\n",
    "                        new_trailing_whitespace.extend([False, False, row[\"trailing_whitespace\"][i]])\n",
    "                        new_labels.extend([\"O\", row[\"labels\"][i], \"O\"])\n",
    "\n",
    "        new_full_text = \"\".join([token + \" \" * space for token, space in zip(new_tokens, new_trailing_whitespace)])\n",
    "\n",
    "        if len(new_tokens) != len(new_trailing_whitespace) or len(new_trailing_whitespace) != len(new_labels):\n",
    "            print(\"The lengths of different columns are not equal!\")\n",
    "\n",
    "        new_row = pd.DataFrame({\n",
    "            \"document\": [row[\"document\"]],\n",
    "            \"full_text\": [new_full_text],\n",
    "            \"tokens\": [list(new_tokens)],\n",
    "            \"trailing_whitespace\": [list(new_trailing_whitespace)],\n",
    "            \"labels\": [list(new_labels)]\n",
    "        })\n",
    "        aug_df = pd.concat([aug_df, new_row], ignore_index=True)\n",
    "\n",
    "aug_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_df.to_json(\"./noise-openai-faker/lzc_noise_data_1500_0206_augmented.json\", orient=\"records\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
