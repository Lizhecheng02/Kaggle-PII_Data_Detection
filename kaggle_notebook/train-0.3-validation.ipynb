{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import json\n","import argparse\n","import pandas as pd\n","import numpy as np\n","import torch\n","import evaluate\n","import os\n","import random\n","\n","from transformers import AutoTokenizer, Trainer, TrainingArguments\n","from transformers import AutoModelForTokenClassification, DataCollatorForTokenClassification\n","from datasets import Dataset, features, DatasetDict\n","from seqeval.metrics import recall_score, precision_score\n","from seqeval.metrics import classification_report\n","from seqeval.metrics import f1_score\n","from itertools import chain\n","from functools import partial\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["TRAINING_MODEL_PATH = \"microsoft/deberta-v3-large\"\n","TRAINING_MAX_LENGTH = 768  \n","OUTPUT_DIR = \"output\""]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train1 = json.load(open(\"../kaggle_dataset/train_split.json\"))\n","train2 = json.load(open(\"../kaggle_dataset/pjm_gpt_2k_0126_fixed.json\"))\n","train = train1 + train2\n","random.shuffle(train)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test = json.load(open(\"../kaggle_dataset/test_split.json\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(len(train))\n","print(train[0].keys())"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(len(test))\n","print(test[0].keys())"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["all_labels = sorted(list(set(chain(*[x[\"labels\"] for x in test]))))\n","label2id = {l: i for i, l in enumerate(all_labels)}\n","id2label = {v: k for k, v in label2id.items()}\n","\n","print(id2label)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["target = [\n","    \"B-EMAIL\", \"B-ID_NUM\", \"B-NAME_STUDENT\", \"B-PHONE_NUM\", \n","    \"B-STREET_ADDRESS\", \"B-URL_PERSONAL\", \"B-USERNAME\", \"I-ID_NUM\", \n","    \"I-NAME_STUDENT\", \"I-PHONE_NUM\", \"I-STREET_ADDRESS\", \"I-URL_PERSONAL\"\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def tokenize(example, tokenizer, label2id):\n","    text = []\n","\n","    # these are at the character level\n","    labels = []\n","    targets = []\n","\n","    for t, l, ws in zip(example[\"tokens\"], example[\"provided_labels\"], example[\"trailing_whitespace\"]):\n","\n","        text.append(t)\n","        labels.extend([l] * len(t))\n","        \n","        if l in target:\n","            targets.append(1)\n","        else:\n","            targets.append(0)\n","        # if there is trailing whitespace\n","        if ws:\n","            text.append(\" \")\n","            labels.append(\"O\")\n","\n","    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation=True, max_length=TRAINING_MAX_LENGTH)\n","    \n","    target_num = sum(targets)\n","    labels = np.array(labels)\n","\n","    text = \"\".join(text)\n","    token_labels = []\n","\n","    for start_idx, end_idx in tokenized.offset_mapping:\n","\n","        # CLS token\n","        if start_idx == 0 and end_idx == 0: \n","            token_labels.append(label2id[\"O\"])\n","            continue\n","\n","        # case when token starts with whitespace\n","        if text[start_idx].isspace():\n","            start_idx += 1\n","\n","        token_labels.append(label2id[labels[start_idx]])\n","\n","    length = len(tokenized.input_ids)\n","\n","    return {\n","        **tokenized,\n","        \"labels\": token_labels,\n","        \"length\": length,\n","        \"target_num\": target_num,\n","        \"group\": 1 if target_num > 0 else 0\n","    }"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(TRAINING_MODEL_PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_ds = Dataset.from_dict({\n","    \"full_text\": [x[\"full_text\"] for x in train],\n","    \"document\": [str(x[\"document\"]) for x in train],\n","    \"tokens\": [x[\"tokens\"] for x in train],\n","    \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in train],\n","    \"provided_labels\": [x[\"labels\"] for x in train],\n","})"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%time\n","train_ds = train_ds.map(tokenize, fn_kwargs={\"tokenizer\": tokenizer, \"label2id\": label2id}, num_proc=2)\n","train_ds = train_ds.class_encode_column(\"group\")\n","print(train_ds)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_ds = Dataset.from_dict({\n","    \"full_text\": [x[\"full_text\"] for x in test],\n","    \"document\": [str(x[\"document\"]) for x in test],\n","    \"tokens\": [x[\"tokens\"] for x in test],\n","    \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in test],\n","    \"provided_labels\": [x[\"labels\"] for x in test],\n","})"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%time\n","test_ds = test_ds.map(tokenize, fn_kwargs={\"tokenizer\": tokenizer, \"label2id\": label2id}, num_proc=2)\n","test_ds = test_ds.class_encode_column(\"group\")\n","print(test_ds)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["x = train_ds[0]\n","\n","for t, l in zip(x[\"tokens\"], x[\"provided_labels\"]):\n","    if l != \"O\":\n","        print((t, l))\n","\n","print(\"*\" * 100)\n","\n","for t, l in zip(tokenizer.convert_ids_to_tokens(x[\"input_ids\"]), x[\"labels\"]):\n","    if id2label[l] != \"O\":\n","        print((t, id2label[l]))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def compute_metrics(p, all_labels):\n","    predictions, labels = p\n","    predictions = np.argmax(predictions, axis=2)\n","\n","    # Remove ignored index (special tokens)\n","    true_predictions = [\n","        [all_labels[p] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","    true_labels = [\n","        [all_labels[l] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","    \n","    recall = recall_score(true_labels, true_predictions)\n","    precision = precision_score(true_labels, true_predictions)\n","    f1_score = (1 + 5 * 5) * recall * precision / (5 * 5 * precision + recall)\n","    \n","    results = {\n","        \"recall\": recall,\n","        \"precision\": precision,\n","        \"f1\": f1_score\n","    }\n","    return results"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model = AutoModelForTokenClassification.from_pretrained(\n","    TRAINING_MODEL_PATH,\n","    num_labels=len(all_labels),\n","    id2label=id2label,\n","    label2id=label2id,\n","    ignore_mismatched_sizes=True\n",")\n","collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["FREEZE_EMBEDDINGS = False\n","FREEZE_LAYERS = 0\n","\n","if FREEZE_EMBEDDINGS:\n","    print(\"Freezing embeddings.\")\n","    for param in model.deberta.embeddings.parameters():\n","        param.requires_grad = False\n","        \n","if FREEZE_LAYERS > 0:\n","    print(f\"Freezing {FREEZE_LAYERS} layers.\")\n","    for layer in model.deberta.encoder.layer[:FREEZE_LAYERS]:\n","        for param in layer.parameters():\n","            param.requires_grad = False"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["final_ds = DatasetDict({\n","    \"train\": train_ds,\n","    \"test\": test_ds\n","})\n","\n","print(final_ds)"]},{"cell_type":"markdown","metadata":{},"source":["## training"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["args = TrainingArguments(\n","    output_dir=OUTPUT_DIR, \n","    fp16=True,\n","    warmup_steps=25,\n","    learning_rate=1e-5,\n","    num_train_epochs=3,\n","    per_device_train_batch_size=1,\n","    per_device_eval_batch_size=1,\n","    report_to=\"none\",\n","    gradient_accumulation_steps=16,\n","    logging_steps=50,\n","    evaluation_strategy=\"steps\",\n","    eval_steps=50,\n","    save_strategy=\"steps\",\n","    save_steps=50,\n","    save_total_limit=6,\n","    save_only_model=True,\n","    overwrite_output_dir=True,\n","    load_best_model_at_end=True,\n","    lr_scheduler_type=\"linear\",\n","    metric_for_best_model=\"f1\",\n","    greater_is_better=True,\n","    weight_decay=0.001\n",")\n","trainer = Trainer(\n","    model=model, \n","    args=args, \n","    train_dataset=final_ds[\"train\"], \n","    eval_dataset=final_ds[\"test\"], \n","    data_collator=collator, \n","    tokenizer=tokenizer,\n","    compute_metrics=partial(compute_metrics, all_labels=all_labels),\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%time\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["trainer.save_model(\"final\")\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def delete_optimizer_files(directory):\n","    for root, dirs, files in os.walk(directory):\n","        for file in files:\n","            if file == \"optimizer.pt\":\n","                os.remove(os.path.join(root, file))\n","                print(f\"Deleted: {os.path.join(root, file)}\")\n","\n","directory_path = \"./\"\n","delete_optimizer_files(directory_path)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":7500999,"sourceId":66653,"sourceType":"competition"},{"datasetId":2210196,"sourceId":3693646,"sourceType":"datasetVersion"},{"datasetId":3958554,"sourceId":6890859,"sourceType":"datasetVersion"},{"datasetId":4353252,"sourceId":7490668,"sourceType":"datasetVersion"}],"dockerImageVersionId":30636,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":4}
