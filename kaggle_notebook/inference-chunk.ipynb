{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07fcaea2",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-01-31T19:29:09.948431Z",
     "iopub.status.busy": "2024-01-31T19:29:09.947704Z",
     "iopub.status.idle": "2024-01-31T19:29:30.081362Z",
     "shell.execute_reply": "2024-01-31T19:29:30.080539Z"
    },
    "papermill": {
     "duration": 20.140741,
     "end_time": "2024-01-31T19:29:30.083622",
     "exception": false,
     "start_time": "2024-01-31T19:29:09.942881",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments, DataCollatorForTokenClassification\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14c7fbd3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-31T19:29:30.092500Z",
     "iopub.status.busy": "2024-01-31T19:29:30.091423Z",
     "iopub.status.idle": "2024-01-31T19:29:30.095783Z",
     "shell.execute_reply": "2024-01-31T19:29:30.094965Z"
    },
    "papermill": {
     "duration": 0.01058,
     "end_time": "2024-01-31T19:29:30.097685",
     "exception": false,
     "start_time": "2024-01-31T19:29:30.087105",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "INFERENCE_MAX_LENGTH = 768\n",
    "TRAINING_MODEL_PATH = \"/kaggle/input/pii-data-detection-models/checkpoint-1250/checkpoint-1250\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7116cf7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-31T19:29:30.104926Z",
     "iopub.status.busy": "2024-01-31T19:29:30.104646Z",
     "iopub.status.idle": "2024-01-31T19:29:30.118839Z",
     "shell.execute_reply": "2024-01-31T19:29:30.118038Z"
    },
    "papermill": {
     "duration": 0.020183,
     "end_time": "2024-01-31T19:29:30.120906",
     "exception": false,
     "start_time": "2024-01-31T19:29:30.100723",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = json.load(open(Path(TRAINING_MODEL_PATH) / \"config.json\"))\n",
    "\n",
    "id2label = {\n",
    "    0: \"B-EMAIL\",\n",
    "    1: \"B-ID_NUM\",\n",
    "    2: \"B-NAME_STUDENT\",\n",
    "    3: \"B-PHONE_NUM\",\n",
    "    4: \"B-STREET_ADDRESS\",\n",
    "    5: \"B-URL_PERSONAL\",\n",
    "    6: \"B-USERNAME\",\n",
    "    7: \"I-ID_NUM\",\n",
    "    8: \"I-NAME_STUDENT\",\n",
    "    9: \"I-PHONE_NUM\",\n",
    "    10: \"I-STREET_ADDRESS\",\n",
    "    11: \"I-URL_PERSONAL\",\n",
    "    12: \"O\"\n",
    "}\n",
    "\n",
    "label2id = {\n",
    "    \"B-EMAIL\": 0,\n",
    "    \"B-ID_NUM\": 1,\n",
    "    \"B-NAME_STUDENT\": 2,\n",
    "    \"B-PHONE_NUM\": 3,\n",
    "    \"B-STREET_ADDRESS\": 4,\n",
    "    \"B-URL_PERSONAL\": 5,\n",
    "    \"B-USERNAME\": 6,\n",
    "    \"I-ID_NUM\": 7,\n",
    "    \"I-NAME_STUDENT\": 8,\n",
    "    \"I-PHONE_NUM\": 9,\n",
    "    \"I-STREET_ADDRESS\": 10,\n",
    "    \"I-URL_PERSONAL\": 11,\n",
    "    \"O\": 12\n",
    "}\n",
    "\n",
    "all_labels = [\n",
    "    \"B-EMAIL\",\n",
    "    \"B-ID_NUM\",\n",
    "    \"B-NAME_STUDENT\",\n",
    "    \"B-PHONE_NUM\",\n",
    "    \"B-STREET_ADDRESS\",\n",
    "    \"B-URL_PERSONAL\",\n",
    "    \"B-USERNAME\",\n",
    "    \"I-ID_NUM\",\n",
    "    \"I-NAME_STUDENT\",\n",
    "    \"I-PHONE_NUM\",\n",
    "    \"I-STREET_ADDRESS\",\n",
    "    \"I-URL_PERSONAL\",\n",
    "    \"O\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "817d9164",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-31T19:29:30.128674Z",
     "iopub.status.busy": "2024-01-31T19:29:30.128153Z",
     "iopub.status.idle": "2024-01-31T19:29:30.161917Z",
     "shell.execute_reply": "2024-01-31T19:29:30.160948Z"
    },
    "papermill": {
     "duration": 0.040022,
     "end_time": "2024-01-31T19:29:30.164186",
     "exception": false,
     "start_time": "2024-01-31T19:29:30.124164",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_json(\"../kaggle_dataset/competition/test.json\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(TRAINING_MODEL_PATH)\n",
    "\n",
    "def get_labels(word_ids, word_labels):\n",
    "    label_ids = []\n",
    "    for word_idx in word_ids:                            \n",
    "        if word_idx is None:\n",
    "            label_ids.append(-100)\n",
    "        else:\n",
    "            label_ids.append(label2id[word_labels[word_idx]])\n",
    "    return label_ids\n",
    "\n",
    "# Tokenize texts, possibly generating more than one tokenized sample for each text\n",
    "def tokenize(df, to_tensor=True, with_labels=True):\n",
    "    \n",
    "    # This is what\"s different from a longformer\n",
    "    # Read the parameters with attention\n",
    "    encoded = tokenizer(\n",
    "        df[\"tokens\"].tolist(),\n",
    "        is_split_into_words=True,\n",
    "        return_overflowing_tokens=True,\n",
    "        stride=0,\n",
    "        max_length=INFERENCE_MAX_LENGTH,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    if with_labels:\n",
    "        encoded[\"labels\"] = []\n",
    "\n",
    "    encoded[\"wids\"] = []\n",
    "    n = len(encoded[\"overflow_to_sample_mapping\"])\n",
    "    for i in range(n):\n",
    "\n",
    "        # Map back to original row\n",
    "        text_idx = encoded[\"overflow_to_sample_mapping\"][i]\n",
    "        \n",
    "        # Get word indexes (this is a global index that takes into consideration the chunking :D )\n",
    "        word_ids = encoded.word_ids(i)\n",
    "        \n",
    "        if with_labels:\n",
    "            # Get word labels of the full un-chunked text\n",
    "            word_labels = df[\"labels\"].iloc[text_idx]\n",
    "        \n",
    "            # Get the labels associated with the word indexes\n",
    "            label_ids = get_labels(word_ids, word_labels)\n",
    "            encoded[\"labels\"].append(label_ids)\n",
    "            \n",
    "        encoded[\"wids\"].append([w if w is not None else -1 for w in word_ids])\n",
    "    \n",
    "    if to_tensor:\n",
    "        encoded = {key: torch.as_tensor(val) for key, val in encoded.items()}\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95957c10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-31T19:29:30.174729Z",
     "iopub.status.busy": "2024-01-31T19:29:30.174439Z",
     "iopub.status.idle": "2024-01-31T19:29:41.512089Z",
     "shell.execute_reply": "2024-01-31T19:29:41.511196Z"
    },
    "papermill": {
     "duration": 11.344362,
     "end_time": "2024-01-31T19:29:41.514441",
     "exception": false,
     "start_time": "2024-01-31T19:29:30.170079",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PIIDataset(Dataset):\n",
    "    def __init__(self, tokenized_ds):\n",
    "        self.data = tokenized_ds\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = {k: self.data[k][index] for k in self.data.keys()}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data[\"input_ids\"])\n",
    "\n",
    "tokenized_test = tokenize(df, with_labels=False)\n",
    "test_dataset = PIIDataset(tokenized_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1)\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    TRAINING_MODEL_PATH,\n",
    "    num_labels=len(all_labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7868a015",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-31T19:29:41.522632Z",
     "iopub.status.busy": "2024-01-31T19:29:41.522319Z",
     "iopub.status.idle": "2024-01-31T19:29:42.269553Z",
     "shell.execute_reply": "2024-01-31T19:29:42.268534Z"
    },
    "papermill": {
     "duration": 0.753929,
     "end_time": "2024-01-31T19:29:42.271994",
     "exception": false,
     "start_time": "2024-01-31T19:29:41.518065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DebertaV2ForTokenClassification(\n",
       "  (deberta): DebertaV2Model(\n",
       "    (embeddings): DebertaV2Embeddings(\n",
       "      (word_embeddings): Embedding(128100, 1024, padding_idx=0)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): StableDropout()\n",
       "    )\n",
       "    (encoder): DebertaV2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(512, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=1024, out_features=13, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b906984",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-31T19:29:42.281234Z",
     "iopub.status.busy": "2024-01-31T19:29:42.280905Z",
     "iopub.status.idle": "2024-01-31T19:29:42.293272Z",
     "shell.execute_reply": "2024-01-31T19:29:42.292420Z"
    },
    "papermill": {
     "duration": 0.019507,
     "end_time": "2024-01-31T19:29:42.295393",
     "exception": false,
     "start_time": "2024-01-31T19:29:42.275886",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inference(df, dl):\n",
    "    \n",
    "    # These 2 dictionaries will hold text-level data\n",
    "    # Helping in the merging process by accumulating data\n",
    "    # Through all the chunks\n",
    "\n",
    "    seen_words_idx = defaultdict(set)\n",
    "    \n",
    "    document, token, label, token_str = [], [], [], []\n",
    "    \n",
    "    for batch in tqdm(dl):\n",
    "        ids = batch[\"input_ids\"].to(\"cuda\")\n",
    "        mask = batch[\"attention_mask\"].to(\"cuda\")\n",
    "        \n",
    "        preds = model(ids, attention_mask=mask, return_dict=False)[0].cpu().detach().numpy() \n",
    "        pred_softmax = np.exp(preds) / np.sum(np.exp(preds), axis=2).reshape(preds.shape[0], preds.shape[1], 1)\n",
    "        preds = preds.argmax(-1)\n",
    "        preds_without_O = pred_softmax[:, :, :12].argmax(-1)\n",
    "        O_preds = pred_softmax[:, :, 12]\n",
    "        threshold = 0.9\n",
    "        preds_final = np.where(O_preds < threshold, preds_without_O, preds)\n",
    "\n",
    "        del ids, mask\n",
    "    \n",
    "        # Go over each prediction, getting the text_id reference\n",
    "        \n",
    "        for k, (chunk_preds, text_id) in enumerate(zip(preds_final, batch[\"overflow_to_sample_mapping\"].tolist())):\n",
    "            # The word_ids are absolute references in the original text\n",
    "            word_ids = batch[\"wids\"][k].numpy()\n",
    "            \n",
    "            # Map from ids to labels\n",
    "            chunk_preds = [id2label[i] for i in chunk_preds]        \n",
    "            \n",
    "            for idx, word_idx in enumerate(word_ids):                            \n",
    "                if word_idx != -1 and chunk_preds[idx] != \"O\" and word_idx not in seen_words_idx[text_id]:\n",
    "                    document.append(df.loc[text_id, \"document\"])\n",
    "                    token.append(word_idx)\n",
    "                    token_str.append(df.loc[text_id, \"tokens\"][word_idx])\n",
    "                    label.append(chunk_preds[idx])\n",
    "                    seen_words_idx[text_id].add(word_idx)\n",
    "                    \n",
    "    df = pd.DataFrame({\n",
    "        \"document\": document,\n",
    "        \"token\": token,\n",
    "        \"label\": label,\n",
    "        \"token_str\": token_str\n",
    "    })\n",
    "    df[\"row_id\"] = list(range(len(df)))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a671a6ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-31T19:29:42.303885Z",
     "iopub.status.busy": "2024-01-31T19:29:42.303599Z",
     "iopub.status.idle": "2024-01-31T19:29:46.942437Z",
     "shell.execute_reply": "2024-01-31T19:29:46.941322Z"
    },
    "papermill": {
     "duration": 4.645424,
     "end_time": "2024-01-31T19:29:46.944485",
     "exception": false,
     "start_time": "2024-01-31T19:29:42.299061",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:04<00:00,  3.03it/s]\n"
     ]
    }
   ],
   "source": [
    "pred_df = inference(df, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef8a908d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-31T19:29:46.956195Z",
     "iopub.status.busy": "2024-01-31T19:29:46.955616Z",
     "iopub.status.idle": "2024-01-31T19:29:46.971381Z",
     "shell.execute_reply": "2024-01-31T19:29:46.970698Z"
    },
    "papermill": {
     "duration": 0.023622,
     "end_time": "2024-01-31T19:29:46.973305",
     "exception": false,
     "start_time": "2024-01-31T19:29:46.949683",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_df[[\"row_id\", \"document\", \"token\", \"label\"]].to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 7500999,
     "sourceId": 66653,
     "sourceType": "competition"
    },
    {
     "datasetId": 4352433,
     "sourceId": 7477327,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4364651,
     "isSourceIdPinned": true,
     "sourceId": 7503934,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30636,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 43.724688,
   "end_time": "2024-01-31T19:29:50.008172",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-01-31T19:29:06.283484",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
