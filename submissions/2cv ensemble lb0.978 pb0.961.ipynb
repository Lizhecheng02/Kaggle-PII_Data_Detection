{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eead504f",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-04-23T07:40:57.456520Z",
     "iopub.status.busy": "2024-04-23T07:40:57.455653Z",
     "iopub.status.idle": "2024-04-23T07:41:16.374307Z",
     "shell.execute_reply": "2024-04-23T07:41:16.373352Z"
    },
    "papermill": {
     "duration": 18.932439,
     "end_time": "2024-04-23T07:41:16.376882",
     "exception": false,
     "start_time": "2024-04-23T07:40:57.444443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-23 07:41:07.913567: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-23 07:41:07.913679: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-23 07:41:08.049433: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import argparse\n",
    "import os\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments, DataCollatorForTokenClassification, LongformerTokenizerFast\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bc3e155",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T07:41:16.399263Z",
     "iopub.status.busy": "2024-04-23T07:41:16.398678Z",
     "iopub.status.idle": "2024-04-23T07:41:16.405533Z",
     "shell.execute_reply": "2024-04-23T07:41:16.404820Z"
    },
    "papermill": {
     "duration": 0.020124,
     "end_time": "2024-04-23T07:41:16.407505",
     "exception": false,
     "start_time": "2024-04-23T07:41:16.387381",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "INFERENCE_STRIDE = 256\n",
    "\n",
    "INFERENCE_MAX_LENGTH_NO_REPLACE = [896, 896, 896, 896]\n",
    "TRAINING_MODEL_PATH_NO_REPLACE = [\n",
    "    \"/kaggle/input/kfold-ex-6-avg-0-9757/fold0/model_0.9745/checkpoint-2500\",\n",
    "    \"/kaggle/input/kfold-ex-6-avg-0-9757/fold1/model_0.9754/checkpoint-2600\",\n",
    "    \"/kaggle/input/kfold-ex-6-avg-0-9757/fold2/model_0.9794/checkpoint-2600\",\n",
    "    \"/kaggle/input/kfold-ex-6-avg-0-9757/fold3/model_0.9735/checkpoint-1900\"\n",
    "]\n",
    "\n",
    "INFERENCE_MAX_LENGTH_REPLACE = [896, 896, 896, 896]\n",
    "TRAINING_MODEL_PATH_REPLACE = [\n",
    "    \"/kaggle/input/kfold-ex-18-avg-0-97735-replace-nn/fold0/model_0.9794/checkpoint-2100\",\n",
    "    \"/kaggle/input/kfold-ex-18-avg-0-97735-replace-nn/fold1/model_0.9787/checkpoint-2100\",\n",
    "    \"/kaggle/input/kfold-ex-18-avg-0-97735-replace-nn/fold2/model_0.9756/checkpoint-2300\",\n",
    "    \"/kaggle/input/kfold-ex-18-avg-0-97735-replace-nn/fold3/model_0.9757/checkpoint-2000\"\n",
    "]  # 替换\\n\\n的\n",
    "\n",
    "TRAINING_MODEL_PATH = TRAINING_MODEL_PATH_NO_REPLACE + TRAINING_MODEL_PATH_REPLACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e63d2e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T07:41:16.428182Z",
     "iopub.status.busy": "2024-04-23T07:41:16.427879Z",
     "iopub.status.idle": "2024-04-23T07:41:16.434638Z",
     "shell.execute_reply": "2024-04-23T07:41:16.433756Z"
    },
    "papermill": {
     "duration": 0.019315,
     "end_time": "2024-04-23T07:41:16.436660",
     "exception": false,
     "start_time": "2024-04-23T07:41:16.417345",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: 'B-EMAIL',\n",
    "    1: 'B-ID_NUM',\n",
    "    2: 'B-NAME_STUDENT',\n",
    "    3: 'B-PHONE_NUM',\n",
    "    4: 'B-STREET_ADDRESS',\n",
    "    5: 'B-URL_PERSONAL',\n",
    "    6: 'B-USERNAME',\n",
    "    7: 'I-ID_NUM',\n",
    "    8: 'I-NAME_STUDENT',\n",
    "    9: 'I-PHONE_NUM',\n",
    "    10: 'I-STREET_ADDRESS',\n",
    "    11: 'I-URL_PERSONAL',\n",
    "    12: 'O'\n",
    "}\n",
    "label2id = {\n",
    "    'B-EMAIL': 0,\n",
    "    'B-ID_NUM': 1,\n",
    "    'B-NAME_STUDENT': 2,\n",
    "    'B-PHONE_NUM': 3,\n",
    "    'B-STREET_ADDRESS': 4,\n",
    "    'B-URL_PERSONAL': 5,\n",
    "    'B-USERNAME': 6,\n",
    "    'I-ID_NUM': 7,\n",
    "    'I-NAME_STUDENT': 8,\n",
    "    'I-PHONE_NUM': 9,\n",
    "    'I-STREET_ADDRESS': 10,\n",
    "    'I-URL_PERSONAL': 11,\n",
    "    'O': 12\n",
    "}\n",
    "all_labels = [\n",
    "    'B-EMAIL',\n",
    "    'B-ID_NUM',\n",
    "    'B-NAME_STUDENT',\n",
    "    'B-PHONE_NUM',\n",
    "    'B-STREET_ADDRESS',\n",
    "    'B-URL_PERSONAL',\n",
    "    'B-USERNAME',\n",
    "    'I-ID_NUM',\n",
    "    'I-NAME_STUDENT',\n",
    "    'I-PHONE_NUM',\n",
    "    'I-STREET_ADDRESS',\n",
    "    'I-URL_PERSONAL',\n",
    "    'O'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f13a7a44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T07:41:16.457652Z",
     "iopub.status.busy": "2024-04-23T07:41:16.457360Z",
     "iopub.status.idle": "2024-04-23T07:41:16.469231Z",
     "shell.execute_reply": "2024-04-23T07:41:16.468542Z"
    },
    "papermill": {
     "duration": 0.024453,
     "end_time": "2024-04-23T07:41:16.471125",
     "exception": false,
     "start_time": "2024-04-23T07:41:16.446672",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_labels(word_ids, word_labels):\n",
    "    label_ids = []\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            label_ids.append(-100)\n",
    "        else:\n",
    "            label_ids.append(label2id[word_labels[word_idx]])\n",
    "    return label_ids\n",
    "\n",
    "# Tokenize texts, possibly generating more than one tokenized sample for each text\n",
    "\n",
    "\n",
    "def tokenize(df, tokenizer, inference_max_length, to_tensor=True, with_labels=True):\n",
    "\n",
    "    # This is what's different from a longformer\n",
    "    # Read the parameters with attention\n",
    "    encoded = tokenizer(df['tokens'].tolist(),\n",
    "                        is_split_into_words=True,\n",
    "                        return_overflowing_tokens=True,\n",
    "                        stride=INFERENCE_STRIDE,\n",
    "                        max_length=inference_max_length,\n",
    "                        padding=\"max_length\",\n",
    "                        truncation=True)\n",
    "\n",
    "    if with_labels:\n",
    "        encoded['labels'] = []\n",
    "\n",
    "    encoded['wids'] = []\n",
    "    n = len(encoded['overflow_to_sample_mapping'])\n",
    "    for i in range(n):\n",
    "\n",
    "        # Map back to original row\n",
    "        text_idx = encoded['overflow_to_sample_mapping'][i]\n",
    "\n",
    "        # Get word indexes (this is a global index that takes into consideration the chunking :D )\n",
    "        word_ids = encoded.word_ids(i)\n",
    "\n",
    "        if with_labels:\n",
    "            # Get word labels of the full un-chunked text\n",
    "            word_labels = df['labels'].iloc[text_idx]\n",
    "\n",
    "            # Get the labels associated with the word indexes\n",
    "            label_ids = get_labels(word_ids, word_labels)\n",
    "            encoded['labels'].append(label_ids)\n",
    "        encoded['wids'].append([w if w is not None else -1 for w in word_ids])\n",
    "\n",
    "    if to_tensor:\n",
    "        encoded = {key: torch.as_tensor(val) for key, val in encoded.items()}\n",
    "    return encoded\n",
    "\n",
    "\n",
    "class PIIDataset(Dataset):\n",
    "    def __init__(self, tokenized_ds):\n",
    "        self.data = tokenized_ds\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = {k: self.data[k][index] for k in self.data.keys()}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6258033",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T07:41:16.492290Z",
     "iopub.status.busy": "2024-04-23T07:41:16.492018Z",
     "iopub.status.idle": "2024-04-23T07:41:16.501977Z",
     "shell.execute_reply": "2024-04-23T07:41:16.501070Z"
    },
    "papermill": {
     "duration": 0.022784,
     "end_time": "2024-04-23T07:41:16.503960",
     "exception": false,
     "start_time": "2024-04-23T07:41:16.481176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inferenceV4(df, dl, model, gpu_id):\n",
    "\n",
    "    # These 2 dictionaries will hold text-level data\n",
    "    # Helping in the merging process by accumulating data\n",
    "    # Through all the chunks\n",
    "\n",
    "    token_pred = defaultdict(lambda: defaultdict(int))\n",
    "    token_cnt = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    for batch in dl:\n",
    "        ids = batch[\"input_ids\"].to(devices[gpu_id])\n",
    "        mask = batch[\"attention_mask\"].to(devices[gpu_id])\n",
    "        preds = model(ids, attention_mask=mask, return_dict=False)[0].cpu().detach().numpy()\n",
    "        preds_softmax = np.exp(preds) / np.sum(np.exp(preds), axis=2).reshape(preds.shape[0], preds.shape[1], 1)\n",
    "\n",
    "        del ids, mask\n",
    "\n",
    "        # Go over each prediction, getting the text_id reference\n",
    "\n",
    "        for k, (chunk_preds, text_id) in enumerate(zip(preds_softmax, batch['overflow_to_sample_mapping'].tolist())):\n",
    "            # The word_ids are absolute references in the original text\n",
    "            word_ids = batch['wids'][k].numpy()\n",
    "\n",
    "            for idx, word_idx in enumerate(word_ids):\n",
    "                if word_idx != -1:\n",
    "                    token_pred[text_id][word_idx] += chunk_preds[idx]\n",
    "                    token_cnt[text_id][word_idx] += 1\n",
    "\n",
    "    for text_id in token_pred:\n",
    "        for word_idx in token_pred[text_id]:\n",
    "            token_pred[text_id][word_idx] /= token_cnt[text_id][word_idx]\n",
    "\n",
    "    return token_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abb54060",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T07:41:16.525991Z",
     "iopub.status.busy": "2024-04-23T07:41:16.525685Z",
     "iopub.status.idle": "2024-04-23T07:41:16.532613Z",
     "shell.execute_reply": "2024-04-23T07:41:16.531754Z"
    },
    "papermill": {
     "duration": 0.019988,
     "end_time": "2024-04-23T07:41:16.534632",
     "exception": false,
     "start_time": "2024-04-23T07:41:16.514644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_dict_tensor(input_dict):\n",
    "    split_dict_1 = {}\n",
    "    split_dict_2 = {}\n",
    "\n",
    "    for key, value in input_dict.items():\n",
    "        # 如果value不是张量，则尝试将其转换为张量\n",
    "        if not isinstance(value, torch.Tensor):\n",
    "            value = torch.tensor(value)\n",
    "\n",
    "        # 获取当前value的第一个维度长度\n",
    "        split_index = value.shape[0] // 2\n",
    "\n",
    "        # 如果value至少有一个元素，则尝试拆分\n",
    "        if value.shape[0] > 0:\n",
    "            split_value_1, split_value_2 = torch.split(tensor=value, split_size_or_sections=[split_index, value.shape[0] - split_index], dim=0)\n",
    "            split_dict_1[key] = split_value_1\n",
    "            split_dict_2[key] = split_value_2\n",
    "        else:\n",
    "            # 如果value为空，直接复制\n",
    "            split_dict_1[key] = value\n",
    "            split_dict_2[key] = value.clone()  # 确保是一个新的副本\n",
    "\n",
    "    return split_dict_1, split_dict_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "500bdba1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T07:41:16.556138Z",
     "iopub.status.busy": "2024-04-23T07:41:16.555822Z",
     "iopub.status.idle": "2024-04-23T07:41:16.817832Z",
     "shell.execute_reply": "2024-04-23T07:41:16.816843Z"
    },
    "papermill": {
     "duration": 0.275051,
     "end_time": "2024-04-23T07:41:16.820029",
     "exception": false,
     "start_time": "2024-04-23T07:41:16.544978",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LIBRARIES TO CLEAN MEMORY\n",
    "import ctypes\n",
    "import gc\n",
    "import threading\n",
    "import time\n",
    "libc = ctypes.CDLL(\"libc.so.6\")\n",
    "_ = gc.collect()\n",
    "libc.malloc_trim(0)\n",
    "device0 = torch.device(\"cuda:0\")\n",
    "device1 = torch.device(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a15ede57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T07:41:16.842571Z",
     "iopub.status.busy": "2024-04-23T07:41:16.841791Z",
     "iopub.status.idle": "2024-04-23T07:43:00.495314Z",
     "shell.execute_reply": "2024-04-23T07:43:00.494470Z"
    },
    "papermill": {
     "duration": 103.667156,
     "end_time": "2024-04-23T07:43:00.497780",
     "exception": false,
     "start_time": "2024-04-23T07:41:16.830624",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "=> Inferring /kaggle/input/kfold-ex-18-avg-0-97735-replace-nn/fold0/model_0.9794/checkpoint-2100\n",
      "Thread 0 started on GPU 0\n",
      "Thread 1 started on GPU 1\n",
      "Thread 1 finished on GPU 1\n",
      "Thread 0 finished on GPU 0\n",
      "Both threads have finished.\n",
      "\n",
      "#########################\n",
      "=> Inferring /kaggle/input/kfold-ex-18-avg-0-97735-replace-nn/fold1/model_0.9787/checkpoint-2100\n",
      "Thread 0 started on GPU 0\n",
      "Thread 1 started on GPU 1\n",
      "Thread 0 finished on GPU 0\n",
      "Thread 1 finished on GPU 1\n",
      "Both threads have finished.\n",
      "\n",
      "#########################\n",
      "=> Inferring /kaggle/input/kfold-ex-18-avg-0-97735-replace-nn/fold2/model_0.9756/checkpoint-2300\n",
      "Thread 0 started on GPU 0\n",
      "Thread 1 started on GPU 1\n",
      "Thread 0 finished on GPU 0\n",
      "Thread 1 finished on GPU 1\n",
      "Both threads have finished.\n",
      "\n",
      "#########################\n",
      "=> Inferring /kaggle/input/kfold-ex-18-avg-0-97735-replace-nn/fold3/model_0.9757/checkpoint-2000\n",
      "Thread 0 started on GPU 0\n",
      "Thread 1 started on GPU 1\n",
      "Thread 0 finished on GPU 0\n",
      "Thread 1 finished on GPU 1\n",
      "Both threads have finished.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json(\"/kaggle/input/pii-detection-removal-from-educational-data/test.json\")\n",
    "df.tokens = df.tokens.apply(lambda x: [\"|\" if i == '\\n\\n' else i for i in x])\n",
    "\n",
    "final_token_pred = defaultdict(lambda: defaultdict(int))\n",
    "'''\n",
    "有点小问题，可能切分的时候会把一个文档的切成两个部分去推理，这样重合的部分的word没有很好的平均概率的时候，可能会有点问题\n",
    "'''\n",
    "for idx, model_path in enumerate(TRAINING_MODEL_PATH_REPLACE):\n",
    "\n",
    "    print('#'*25)\n",
    "    print('=> Inferring', model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model0 = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_path,\n",
    "        num_labels=len(all_labels),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "        ignore_mismatched_sizes=True\n",
    "    ).to(device0)\n",
    "    model1 = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_path,\n",
    "        num_labels=len(all_labels),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "        ignore_mismatched_sizes=True\n",
    "    ).to(device1)\n",
    "    models = [model0, model1]\n",
    "    devices = [device0, device1]\n",
    "\n",
    "    # 这个tokenize成功运行\n",
    "    tokenized_test = tokenize(\n",
    "        df=df, \n",
    "        inference_max_length=INFERENCE_MAX_LENGTH_REPLACE[idx], \n",
    "        with_labels=False, \n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    last_shape = list(tokenized_test.values())[0].shape[0]\n",
    "    for k, v in tokenized_test.items():\n",
    "        assert last_shape == v.shape[0]\n",
    "        last_shape = v.shape[0]\n",
    "\n",
    "    # 这个split成功运行\n",
    "    sub_df_1, sub_df_2 = split_dict_tensor(tokenized_test)  # 问题所在\n",
    "    # Create a lock to synchronize the threads\n",
    "    lock = threading.Lock()\n",
    "\n",
    "    single_pred = []\n",
    "    # Define a function for inference\n",
    "\n",
    "    def inference_thread(gpu_id, lock, tokenized_test):\n",
    "        with lock:\n",
    "            print(f\"Thread {gpu_id} started on GPU {gpu_id}\")\n",
    "        # 这里也没有问题\n",
    "        test_dataset = PIIDataset(tokenized_test)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=1)\n",
    "\n",
    "        token_pred = inferenceV4(df=df, dl=test_dataloader, model=models[gpu_id], gpu_id=gpu_id)\n",
    "        with lock:\n",
    "            print(f\"Thread {gpu_id} finished on GPU {gpu_id}\")\n",
    "\n",
    "        single_pred.append(token_pred)\n",
    "\n",
    "    # Create two threads for inference\n",
    "    thread1 = threading.Thread(target=inference_thread, args=(0, lock, sub_df_1))\n",
    "    thread2 = threading.Thread(target=inference_thread, args=(1, lock, sub_df_2))\n",
    "\n",
    "    # Start the threads\n",
    "    thread1.start()\n",
    "    thread2.start()\n",
    "\n",
    "    # Wait for both threads to finish\n",
    "    thread1.join()\n",
    "    thread2.join()\n",
    "\n",
    "    print(\"Both threads have finished.\")\n",
    "    print()\n",
    "    for tmp_pred in single_pred:\n",
    "        for text_id in tmp_pred:\n",
    "            for word_idx in tmp_pred[text_id]:\n",
    "                final_token_pred[text_id][word_idx] += tmp_pred[text_id][word_idx] / len(TRAINING_MODEL_PATH)\n",
    "\n",
    "    # CLEAN MEMORY\n",
    "    del model0, model1, models, tokenizer\n",
    "    torch.cuda.empty_cache()\n",
    "    _ = gc.collect()\n",
    "    libc.malloc_trim(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6770191",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T07:43:00.521361Z",
     "iopub.status.busy": "2024-04-23T07:43:00.521022Z",
     "iopub.status.idle": "2024-04-23T07:44:37.810558Z",
     "shell.execute_reply": "2024-04-23T07:44:37.809530Z"
    },
    "papermill": {
     "duration": 97.30405,
     "end_time": "2024-04-23T07:44:37.813051",
     "exception": false,
     "start_time": "2024-04-23T07:43:00.509001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "=> Inferring /kaggle/input/kfold-ex-6-avg-0-9757/fold0/model_0.9745/checkpoint-2500\n",
      "Thread 0 started on GPU 0\n",
      "Thread 1 started on GPU 1\n",
      "Thread 0 finished on GPU 0\n",
      "Thread 1 finished on GPU 1\n",
      "Both threads have finished.\n",
      "\n",
      "#########################\n",
      "=> Inferring /kaggle/input/kfold-ex-6-avg-0-9757/fold1/model_0.9754/checkpoint-2600\n",
      "Thread 0 started on GPU 0\n",
      "Thread 1 started on GPU 1\n",
      "Thread 0 finished on GPU 0\n",
      "Thread 1 finished on GPU 1\n",
      "Both threads have finished.\n",
      "\n",
      "#########################\n",
      "=> Inferring /kaggle/input/kfold-ex-6-avg-0-9757/fold2/model_0.9794/checkpoint-2600\n",
      "Thread 0 started on GPU 0\n",
      "Thread 1 started on GPU 1\n",
      "Thread 0 finished on GPU 0\n",
      "Thread 1 finished on GPU 1\n",
      "Both threads have finished.\n",
      "\n",
      "#########################\n",
      "=> Inferring /kaggle/input/kfold-ex-6-avg-0-9757/fold3/model_0.9735/checkpoint-1900\n",
      "Thread 0 started on GPU 0\n",
      "Thread 1 started on GPU 1\n",
      "Thread 0 finished on GPU 0\n",
      "Thread 1 finished on GPU 1\n",
      "Both threads have finished.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json(\"/kaggle/input/pii-detection-removal-from-educational-data/test.json\")\n",
    "'''\n",
    "有点小问题，可能切分的时候会把一个文档的切成两个部分去推理，这样重合的部分的word没有很好的平均概率的时候，可能会有点问题\n",
    "'''\n",
    "for idx, model_path in enumerate(TRAINING_MODEL_PATH_NO_REPLACE):\n",
    "\n",
    "    print('#'*25)\n",
    "    print('=> Inferring', model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model0 = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_path,\n",
    "        num_labels=len(all_labels),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "        ignore_mismatched_sizes=True\n",
    "    ).to(device0)\n",
    "    model1 = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_path,\n",
    "        num_labels=len(all_labels),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "        ignore_mismatched_sizes=True\n",
    "    ).to(device1)\n",
    "    models = [model0, model1]\n",
    "    devices = [device0, device1]\n",
    "\n",
    "    # 这个tokenize成功运行\n",
    "    tokenized_test = tokenize(\n",
    "        df=df, \n",
    "        inference_max_length=INFERENCE_MAX_LENGTH_NO_REPLACE[idx], \n",
    "        with_labels=False, \n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    last_shape = list(tokenized_test.values())[0].shape[0]\n",
    "    for k, v in tokenized_test.items():\n",
    "        assert last_shape == v.shape[0]\n",
    "        last_shape = v.shape[0]\n",
    "\n",
    "    # 这个split成功运行\n",
    "    sub_df_1, sub_df_2 = split_dict_tensor(tokenized_test)  # 问题所在\n",
    "    # Create a lock to synchronize the threads\n",
    "    lock = threading.Lock()\n",
    "\n",
    "    single_pred = []\n",
    "    # Define a function for inference\n",
    "\n",
    "    def inference_thread(gpu_id, lock, tokenized_test):\n",
    "        with lock:\n",
    "            print(f\"Thread {gpu_id} started on GPU {gpu_id}\")\n",
    "        # 这里也没有问题\n",
    "        test_dataset = PIIDataset(tokenized_test)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=1)\n",
    "\n",
    "        token_pred = inferenceV4(df=df, dl=test_dataloader, model=models[gpu_id], gpu_id=gpu_id)\n",
    "        with lock:\n",
    "            print(f\"Thread {gpu_id} finished on GPU {gpu_id}\")\n",
    "\n",
    "        single_pred.append(token_pred)\n",
    "\n",
    "    # Create two threads for inference\n",
    "    thread1 = threading.Thread(target=inference_thread, args=(0, lock, sub_df_1))\n",
    "    thread2 = threading.Thread(target=inference_thread, args=(1, lock, sub_df_2))\n",
    "\n",
    "    # Start the threads\n",
    "    thread1.start()\n",
    "    thread2.start()\n",
    "\n",
    "    # Wait for both threads to finish\n",
    "    thread1.join()\n",
    "    thread2.join()\n",
    "\n",
    "    print(\"Both threads have finished.\")\n",
    "    print()\n",
    "    for tmp_pred in single_pred:\n",
    "        for text_id in tmp_pred:\n",
    "            for word_idx in tmp_pred[text_id]:\n",
    "                final_token_pred[text_id][word_idx] += tmp_pred[text_id][word_idx] / len(TRAINING_MODEL_PATH)\n",
    "\n",
    "    # CLEAN MEMORY\n",
    "    del model0, model1, models, tokenizer\n",
    "    torch.cuda.empty_cache()\n",
    "    _ = gc.collect()\n",
    "    libc.malloc_trim(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a5cc716",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T07:44:37.839085Z",
     "iopub.status.busy": "2024-04-23T07:44:37.838487Z",
     "iopub.status.idle": "2024-04-23T07:44:37.901921Z",
     "shell.execute_reply": "2024-04-23T07:44:37.901056Z"
    },
    "papermill": {
     "duration": 0.078513,
     "end_time": "2024-04-23T07:44:37.904006",
     "exception": false,
     "start_time": "2024-04-23T07:44:37.825493",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "document, token, label, score = [], [], [], []\n",
    "for text_id in final_token_pred:\n",
    "    for word_idx in final_token_pred[text_id]:\n",
    "        pred = final_token_pred[text_id][word_idx].argmax(-1)\n",
    "        pred_without_O = final_token_pred[text_id][word_idx][:12].argmax(-1)\n",
    "        if final_token_pred[text_id][word_idx][12] < 0.0:\n",
    "            final_pred = pred_without_O\n",
    "            tmp_score = final_token_pred[text_id][word_idx][final_pred]\n",
    "\n",
    "        else:\n",
    "            final_pred = pred\n",
    "            tmp_score = final_token_pred[text_id][word_idx][final_pred]\n",
    "\n",
    "        if id2label[final_pred] != 'O':\n",
    "            document.append(df.loc[text_id, \"document\"])\n",
    "            token.append(word_idx)\n",
    "            label.append(id2label[final_pred])\n",
    "            score.append(tmp_score)\n",
    "\n",
    "pred_df = pd.DataFrame({\n",
    "    \"document\": document,\n",
    "    \"token\": token,\n",
    "    \"label\": label,\n",
    "    \"score\": score\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c8637ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T07:44:37.933026Z",
     "iopub.status.busy": "2024-04-23T07:44:37.932680Z",
     "iopub.status.idle": "2024-04-23T07:44:37.947161Z",
     "shell.execute_reply": "2024-04-23T07:44:37.946461Z"
    },
    "papermill": {
     "duration": 0.030526,
     "end_time": "2024-04-23T07:44:37.949060",
     "exception": false,
     "start_time": "2024-04-23T07:44:37.918534",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_df = pred_df.sort_values(['document', 'token']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d2f488b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T07:44:37.973997Z",
     "iopub.status.busy": "2024-04-23T07:44:37.973692Z",
     "iopub.status.idle": "2024-04-23T07:44:37.991334Z",
     "shell.execute_reply": "2024-04-23T07:44:37.990354Z"
    },
    "papermill": {
     "duration": 0.032528,
     "end_time": "2024-04-23T07:44:37.993584",
     "exception": false,
     "start_time": "2024-04-23T07:44:37.961056",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>token</th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "      <td>0.999237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "      <td>0.999578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>482</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "      <td>0.998972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>483</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "      <td>0.999484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>741</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "      <td>0.998966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>742</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "      <td>0.999478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "      <td>0.999310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "      <td>0.999655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>464</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "      <td>0.999031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>465</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "      <td>0.999305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "      <td>0.999585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "      <td>0.999567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "      <td>0.999293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "      <td>0.999298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20</td>\n",
       "      <td>328</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "      <td>0.712413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>20</td>\n",
       "      <td>330</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "      <td>0.676984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>56</td>\n",
       "      <td>12</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "      <td>0.999599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>56</td>\n",
       "      <td>13</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "      <td>0.999519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>86</td>\n",
       "      <td>6</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "      <td>0.999420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>86</td>\n",
       "      <td>7</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "      <td>0.999439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>93</td>\n",
       "      <td>0</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "      <td>0.999467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "      <td>0.999426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>104</td>\n",
       "      <td>7</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "      <td>0.528910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>104</td>\n",
       "      <td>8</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "      <td>0.997555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>104</td>\n",
       "      <td>9</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "      <td>0.999361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>112</td>\n",
       "      <td>5</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "      <td>0.999615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>112</td>\n",
       "      <td>6</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "      <td>0.999638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>123</td>\n",
       "      <td>32</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "      <td>0.999463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>123</td>\n",
       "      <td>33</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "      <td>0.999440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    document  token           label     score\n",
       "0          7      9  B-NAME_STUDENT  0.999237\n",
       "1          7     10  I-NAME_STUDENT  0.999578\n",
       "2          7    482  B-NAME_STUDENT  0.998972\n",
       "3          7    483  I-NAME_STUDENT  0.999484\n",
       "4          7    741  B-NAME_STUDENT  0.998966\n",
       "5          7    742  I-NAME_STUDENT  0.999478\n",
       "6         10      0  B-NAME_STUDENT  0.999310\n",
       "7         10      1  I-NAME_STUDENT  0.999655\n",
       "8         10    464  B-NAME_STUDENT  0.999031\n",
       "9         10    465  I-NAME_STUDENT  0.999305\n",
       "10        16      4  B-NAME_STUDENT  0.999585\n",
       "11        16      5  I-NAME_STUDENT  0.999567\n",
       "12        20      5  B-NAME_STUDENT  0.999293\n",
       "13        20      6  I-NAME_STUDENT  0.999298\n",
       "14        20    328  B-NAME_STUDENT  0.712413\n",
       "15        20    330  B-NAME_STUDENT  0.676984\n",
       "16        56     12  B-NAME_STUDENT  0.999599\n",
       "17        56     13  I-NAME_STUDENT  0.999519\n",
       "18        86      6  B-NAME_STUDENT  0.999420\n",
       "19        86      7  I-NAME_STUDENT  0.999439\n",
       "20        93      0  B-NAME_STUDENT  0.999467\n",
       "21        93      1  I-NAME_STUDENT  0.999426\n",
       "22       104      7  B-NAME_STUDENT  0.528910\n",
       "23       104      8  B-NAME_STUDENT  0.997555\n",
       "24       104      9  I-NAME_STUDENT  0.999361\n",
       "25       112      5  B-NAME_STUDENT  0.999615\n",
       "26       112      6  I-NAME_STUDENT  0.999638\n",
       "27       123     32  B-NAME_STUDENT  0.999463\n",
       "28       123     33  I-NAME_STUDENT  0.999440"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2172fd9",
   "metadata": {
    "papermill": {
     "duration": 0.011691,
     "end_time": "2024-04-23T07:44:38.018283",
     "exception": false,
     "start_time": "2024-04-23T07:44:38.006592",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Postprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d94e0ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T07:44:38.043749Z",
     "iopub.status.busy": "2024-04-23T07:44:38.043140Z",
     "iopub.status.idle": "2024-04-23T07:44:38.049478Z",
     "shell.execute_reply": "2024-04-23T07:44:38.048675Z"
    },
    "papermill": {
     "duration": 0.021255,
     "end_time": "2024-04-23T07:44:38.051408",
     "exception": false,
     "start_time": "2024-04-23T07:44:38.030153",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df[['document', 'tokens']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83de62df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T07:44:38.076936Z",
     "iopub.status.busy": "2024-04-23T07:44:38.076645Z",
     "iopub.status.idle": "2024-04-23T07:44:38.090320Z",
     "shell.execute_reply": "2024-04-23T07:44:38.089385Z"
    },
    "papermill": {
     "duration": 0.02873,
     "end_time": "2024-04-23T07:44:38.092241",
     "exception": false,
     "start_time": "2024-04-23T07:44:38.063511",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.explode(['tokens']).reset_index(drop=True).rename(columns={'tokens': 'token'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ad13ac8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T07:44:38.117490Z",
     "iopub.status.busy": "2024-04-23T07:44:38.117038Z",
     "iopub.status.idle": "2024-04-23T07:44:38.125184Z",
     "shell.execute_reply": "2024-04-23T07:44:38.124427Z"
    },
    "papermill": {
     "duration": 0.022722,
     "end_time": "2024-04-23T07:44:38.126997",
     "exception": false,
     "start_time": "2024-04-23T07:44:38.104275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['token_str'] = df['token']\n",
    "df['token'] = df.groupby('document').cumcount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09a42874",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T07:44:38.152032Z",
     "iopub.status.busy": "2024-04-23T07:44:38.151730Z",
     "iopub.status.idle": "2024-04-23T07:44:38.162409Z",
     "shell.execute_reply": "2024-04-23T07:44:38.161697Z"
    },
    "papermill": {
     "duration": 0.02509,
     "end_time": "2024-04-23T07:44:38.164264",
     "exception": false,
     "start_time": "2024-04-23T07:44:38.139174",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_pred_df = pd.merge(df, pred_df[['document', 'token', 'label', \"score\"]], on=['document', 'token'], how='left')\n",
    "new_pred_df['label'] = new_pred_df['label'].fillna('O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2f238a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T07:44:38.189023Z",
     "iopub.status.busy": "2024-04-23T07:44:38.188745Z",
     "iopub.status.idle": "2024-04-23T07:44:38.203492Z",
     "shell.execute_reply": "2024-04-23T07:44:38.202774Z"
    },
    "papermill": {
     "duration": 0.029329,
     "end_time": "2024-04-23T07:44:38.205373",
     "exception": false,
     "start_time": "2024-04-23T07:44:38.176044",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pp(new_pred_df):\n",
    "    df = new_pred_df.copy()\n",
    "    i = 0\n",
    "    while i < len(df):\n",
    "        st = i\n",
    "        doc = df.loc[st, \"document\"]\n",
    "        tok = df.loc[st, \"token\"]\n",
    "        pred_tok = df.loc[st, \"label\"]\n",
    "        if pred_tok == 'O':\n",
    "            i += 1\n",
    "            continue\n",
    "        lab = pred_tok.split('-')[1]\n",
    "        cur_doc = doc\n",
    "        cur_lab = lab\n",
    "        last_tok = tok\n",
    "        cur_tok = last_tok\n",
    "        # prefix = []\n",
    "        while i < len(df) and cur_doc == doc and cur_lab == lab and last_tok == cur_tok:\n",
    "            # prefix.append(pred_tok.split('-')[0])\n",
    "            last_tok = cur_tok + 1\n",
    "            i += 1\n",
    "            cur_doc = df.loc[i, \"document\"]\n",
    "            cur_tok = df.loc[i, \"token\"]\n",
    "            if i >= len(df) or df.loc[i, \"label\"] == 'O':\n",
    "                break\n",
    "            cur_lab = df.loc[i, \"label\"].split('-')[1]\n",
    "\n",
    "        # exception\n",
    "        if st - 2 >= 0 and df.loc[st - 2, \"document\"] == df.loc[st, \"document\"] and df.loc[st - 1, \"token_str\"] == '\\n' and df.loc[st - 2, \"label\"] != 'O' and df.loc[st - 2, \"label\"].split('-')[1] == lab:\n",
    "            df.loc[st - 1, \"label\"] = 'I-' + lab\n",
    "            df.loc[st - 1, \"score\"] = 1\n",
    "            for j in range(st, i):\n",
    "                if df.loc[j, \"label\"] != 'I-' + lab:\n",
    "                    df.loc[j, \"score\"] = 1\n",
    "                    df.loc[j, \"label\"] = 'I-' + lab\n",
    "            continue\n",
    "\n",
    "        # fix\n",
    "        for j in range(st, i):\n",
    "            if j == st:\n",
    "                if df.loc[j, \"label\"] != 'B-' + lab:\n",
    "                    df.loc[j, \"score\"] = 1\n",
    "                    df.loc[j, \"label\"] = 'B-' + lab\n",
    "            else:\n",
    "                if df.loc[j, \"label\"] != 'I-' + lab:\n",
    "                    df.loc[j, \"score\"] = 1\n",
    "                    df.loc[j, \"label\"] = 'I-' + lab\n",
    "#         print(df.loc[st:i,:])\n",
    "        if lab == 'NAME_STUDENT' and any(len(item) == 2 and item[0].isupper() and item[1] == \".\" for item in df.loc[st:i-1, 'token_str']):\n",
    "            for j in range(st, i):\n",
    "                df.loc[j, \"score\"] = 0\n",
    "                df.loc[j, \"label\"] = 'O'\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6196cd1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T07:44:38.230086Z",
     "iopub.status.busy": "2024-04-23T07:44:38.229790Z",
     "iopub.status.idle": "2024-04-23T07:44:38.638784Z",
     "shell.execute_reply": "2024-04-23T07:44:38.637927Z"
    },
    "papermill": {
     "duration": 0.42381,
     "end_time": "2024-04-23T07:44:38.641008",
     "exception": false,
     "start_time": "2024-04-23T07:44:38.217198",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_pred_df = pp(new_pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aac3d75d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T07:44:38.666371Z",
     "iopub.status.busy": "2024-04-23T07:44:38.666080Z",
     "iopub.status.idle": "2024-04-23T07:44:38.678870Z",
     "shell.execute_reply": "2024-04-23T07:44:38.677877Z"
    },
    "papermill": {
     "duration": 0.027652,
     "end_time": "2024-04-23T07:44:38.680860",
     "exception": false,
     "start_time": "2024-04-23T07:44:38.653208",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>token</th>\n",
       "      <th>token_str</th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>Design</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>Thinking</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>for</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>innovation</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>reflexion</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8500</th>\n",
       "      <td>123</td>\n",
       "      <td>1689</td>\n",
       "      <td>(</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8501</th>\n",
       "      <td>123</td>\n",
       "      <td>1690</td>\n",
       "      <td>https://www.melessa.uni-</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8502</th>\n",
       "      <td>123</td>\n",
       "      <td>1691</td>\n",
       "      <td>muenchen.de/team/vorstandssprecher/schmidt/pub...</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8503</th>\n",
       "      <td>123</td>\n",
       "      <td>1692</td>\n",
       "      <td>)</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8504</th>\n",
       "      <td>123</td>\n",
       "      <td>1693</td>\n",
       "      <td>\\n\\n</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8505 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      document  token                                          token_str  \\\n",
       "0            7      0                                             Design   \n",
       "1            7      1                                           Thinking   \n",
       "2            7      2                                                for   \n",
       "3            7      3                                         innovation   \n",
       "4            7      4                                          reflexion   \n",
       "...        ...    ...                                                ...   \n",
       "8500       123   1689                                                  (   \n",
       "8501       123   1690                           https://www.melessa.uni-   \n",
       "8502       123   1691  muenchen.de/team/vorstandssprecher/schmidt/pub...   \n",
       "8503       123   1692                                                  )   \n",
       "8504       123   1693                                               \\n\\n   \n",
       "\n",
       "     label  score  \n",
       "0        O    NaN  \n",
       "1        O    NaN  \n",
       "2        O    NaN  \n",
       "3        O    NaN  \n",
       "4        O    NaN  \n",
       "...    ...    ...  \n",
       "8500     O    NaN  \n",
       "8501     O    NaN  \n",
       "8502     O    NaN  \n",
       "8503     O    NaN  \n",
       "8504     O    NaN  \n",
       "\n",
       "[8505 rows x 5 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47ac0dc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T07:44:38.706799Z",
     "iopub.status.busy": "2024-04-23T07:44:38.706523Z",
     "iopub.status.idle": "2024-04-23T07:44:38.717022Z",
     "shell.execute_reply": "2024-04-23T07:44:38.716018Z"
    },
    "papermill": {
     "duration": 0.026106,
     "end_time": "2024-04-23T07:44:38.719306",
     "exception": false,
     "start_time": "2024-04-23T07:44:38.693200",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_pred_df = new_pred_df.query(\"label != 'O'\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "efbbfbc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T07:44:38.746290Z",
     "iopub.status.busy": "2024-04-23T07:44:38.746009Z",
     "iopub.status.idle": "2024-04-23T07:44:38.753722Z",
     "shell.execute_reply": "2024-04-23T07:44:38.752935Z"
    },
    "papermill": {
     "duration": 0.022789,
     "end_time": "2024-04-23T07:44:38.755752",
     "exception": false,
     "start_time": "2024-04-23T07:44:38.732963",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows_to_delete = []\n",
    "for idx, row in new_pred_df.iterrows():\n",
    "    if row.label == 'I-PHONE_NUM':\n",
    "        if row.token_str == ')':\n",
    "            rows_to_delete.append(idx)\n",
    "        elif not bool(re.search(r'\\d', row.token_str)):\n",
    "            rows_to_delete.append(idx)\n",
    "    elif row.label == 'B-EMAIL':\n",
    "        if '@' not in row.token_str:\n",
    "            rows_to_delete.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef3acd54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T07:44:38.781024Z",
     "iopub.status.busy": "2024-04-23T07:44:38.780756Z",
     "iopub.status.idle": "2024-04-23T07:44:38.785214Z",
     "shell.execute_reply": "2024-04-23T07:44:38.784313Z"
    },
    "papermill": {
     "duration": 0.019423,
     "end_time": "2024-04-23T07:44:38.787144",
     "exception": false,
     "start_time": "2024-04-23T07:44:38.767721",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_pred_df = new_pred_df.drop(rows_to_delete, axis=0, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d146e0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T07:44:38.812738Z",
     "iopub.status.busy": "2024-04-23T07:44:38.812493Z",
     "iopub.status.idle": "2024-04-23T07:44:38.816911Z",
     "shell.execute_reply": "2024-04-23T07:44:38.816138Z"
    },
    "papermill": {
     "duration": 0.019066,
     "end_time": "2024-04-23T07:44:38.818675",
     "exception": false,
     "start_time": "2024-04-23T07:44:38.799609",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_pred_df[\"row_id\"] = list(range(len(new_pred_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "76a5e9b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T07:44:38.960780Z",
     "iopub.status.busy": "2024-04-23T07:44:38.960525Z",
     "iopub.status.idle": "2024-04-23T07:44:38.968158Z",
     "shell.execute_reply": "2024-04-23T07:44:38.967257Z"
    },
    "papermill": {
     "duration": 0.022699,
     "end_time": "2024-04-23T07:44:38.970140",
     "exception": false,
     "start_time": "2024-04-23T07:44:38.947441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_pred_df[[\"row_id\", \"document\", \"token\", \"label\"]].to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "60bdeb43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T07:44:38.995419Z",
     "iopub.status.busy": "2024-04-23T07:44:38.995086Z",
     "iopub.status.idle": "2024-04-23T07:44:39.009263Z",
     "shell.execute_reply": "2024-04-23T07:44:39.008426Z"
    },
    "papermill": {
     "duration": 0.028965,
     "end_time": "2024-04-23T07:44:39.011252",
     "exception": false,
     "start_time": "2024-04-23T07:44:38.982287",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>document</th>\n",
       "      <th>token</th>\n",
       "      <th>label</th>\n",
       "      <th>token_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "      <td>Nathalie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "      <td>Sylla</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>482</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "      <td>Nathalie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>483</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "      <td>Sylla</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>741</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "      <td>Nathalie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>742</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "      <td>Sylla</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "      <td>Diego</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "      <td>Estrada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>464</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "      <td>Diego</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>465</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "      <td>Estrada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "      <td>Gilberto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "      <td>Gamboa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "      <td>Sindy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "      <td>Samaca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>328</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "      <td>George</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>330</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "      <td>Geoff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>56</td>\n",
       "      <td>12</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "      <td>Nadine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>56</td>\n",
       "      <td>13</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "      <td>Born</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>86</td>\n",
       "      <td>6</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "      <td>Eladio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>86</td>\n",
       "      <td>7</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "      <td>Amaya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>93</td>\n",
       "      <td>0</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "      <td>Silvia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "      <td>Villalobos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>104</td>\n",
       "      <td>7</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "      <td>Dr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>104</td>\n",
       "      <td>8</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "      <td>Sakir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>104</td>\n",
       "      <td>9</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "      <td>Ahmad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>112</td>\n",
       "      <td>5</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "      <td>Francisco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>112</td>\n",
       "      <td>6</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "      <td>Ferreira</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>123</td>\n",
       "      <td>32</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "      <td>Stefano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>123</td>\n",
       "      <td>33</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "      <td>Lovato</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    row_id  document  token           label   token_str\n",
       "0        0         7      9  B-NAME_STUDENT    Nathalie\n",
       "1        1         7     10  I-NAME_STUDENT       Sylla\n",
       "2        2         7    482  B-NAME_STUDENT    Nathalie\n",
       "3        3         7    483  I-NAME_STUDENT       Sylla\n",
       "4        4         7    741  B-NAME_STUDENT    Nathalie\n",
       "5        5         7    742  I-NAME_STUDENT       Sylla\n",
       "6        6        10      0  B-NAME_STUDENT       Diego\n",
       "7        7        10      1  I-NAME_STUDENT     Estrada\n",
       "8        8        10    464  B-NAME_STUDENT       Diego\n",
       "9        9        10    465  I-NAME_STUDENT     Estrada\n",
       "10      10        16      4  B-NAME_STUDENT    Gilberto\n",
       "11      11        16      5  I-NAME_STUDENT      Gamboa\n",
       "12      12        20      5  B-NAME_STUDENT       Sindy\n",
       "13      13        20      6  I-NAME_STUDENT      Samaca\n",
       "14      14        20    328  B-NAME_STUDENT      George\n",
       "15      15        20    330  B-NAME_STUDENT       Geoff\n",
       "16      16        56     12  B-NAME_STUDENT      Nadine\n",
       "17      17        56     13  I-NAME_STUDENT        Born\n",
       "18      18        86      6  B-NAME_STUDENT      Eladio\n",
       "19      19        86      7  I-NAME_STUDENT       Amaya\n",
       "20      20        93      0  B-NAME_STUDENT      Silvia\n",
       "21      21        93      1  I-NAME_STUDENT  Villalobos\n",
       "22      22       104      7  B-NAME_STUDENT          Dr\n",
       "23      23       104      8  I-NAME_STUDENT       Sakir\n",
       "24      24       104      9  I-NAME_STUDENT       Ahmad\n",
       "25      25       112      5  B-NAME_STUDENT   Francisco\n",
       "26      26       112      6  I-NAME_STUDENT    Ferreira\n",
       "27      27       123     32  B-NAME_STUDENT     Stefano\n",
       "28      28       123     33  I-NAME_STUDENT      Lovato"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_pred_df[[\"row_id\", \"document\", \"token\", \"label\", \"token_str\"]]"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 7500999,
     "sourceId": 66653,
     "sourceType": "competition"
    },
    {
     "datasetId": 4385921,
     "sourceId": 7530253,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4415272,
     "sourceId": 7585184,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4441730,
     "sourceId": 7624642,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4491573,
     "sourceId": 7709256,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4447414,
     "sourceId": 7733255,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4617709,
     "sourceId": 7869832,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4661747,
     "sourceId": 7931027,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4667804,
     "sourceId": 7939647,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4781869,
     "sourceId": 8098179,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4807402,
     "sourceId": 8133166,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4828175,
     "sourceId": 8160840,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4832995,
     "sourceId": 8167219,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4854734,
     "sourceId": 8196082,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4782189,
     "sourceId": 8098589,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4782186,
     "sourceId": 8098586,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 227.252642,
   "end_time": "2024-04-23T07:44:41.930110",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-23T07:40:54.677468",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
